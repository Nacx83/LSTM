# -*- coding: utf-8 -*-
"""04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13VzIbquJxPlAcigUevtqSlWbGyJ7dy_4

# Inicio
"""

import os
import warnings
warnings.filterwarnings("ignore")  # oculta todos los warnings de Python
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # oculta warnings de TensorFlow

import os
import tensorflow as tf
import warnings
import subprocess

# Ocultar warnings
warnings.filterwarnings("ignore")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# Mostrar dispositivos disponibles
gpus = tf.config.list_physical_devices('GPU')

if gpus:
    print("‚úÖ GPU detectada por TensorFlow:")
    for i, gpu in enumerate(gpus):
        print(f"  GPU {i}: {gpu.name}")

    # Mostrar informaci√≥n extendida con nvidia-smi (si est√° disponible)
    try:
        print("\nüìä Informaci√≥n de la GPU (nvidia-smi):\n")
        gpu_info = subprocess.check_output(["nvidia-smi"]).decode("utf-8")
        print(gpu_info)
    except FileNotFoundError:
        print("‚ÑπÔ∏è El comando 'nvidia-smi' no se encontr√≥. Puede que est√©s en Colab o sin drivers instalados.")
else:
    print("‚ö†Ô∏è No se detect√≥ ninguna GPU. TensorFlow usar√° CPU.")

"""# ENTORNOS

"""

# !pip install scikeras
# !pip install pandas
# !pip install numpy
# !pip install tensorflow
# !pip install sklearn
# !pip install seaborn

"""#DATASET"""

import pandas as pd
import numpy as np

ruta = r'C:\Users\NaCHO\Mi unidad (martinarjol@gmail.com)\Colab Notebooks\Verano NN\dataset.csv'
ruta2 = '/content/drive/MyDrive/Colab Notebooks/Verano NN/dataset.csv'

df = pd.read_csv(ruta)

df

print(df.columns)

df['Year'] = df['Year'].str[:4].astype(int)

drop_col = ['Air Pollutant Description',
       'Data Aggregation Process', 'Unit Of Air Pollution Level']

df2 = df.drop(columns = drop_col)

df2

df2.info()

df2['Province'].unique()

"""# PREDICCI√ìN DE QUALITY CON TODOS LOS CONTAMINANTES"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GroupKFold
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam

"""#BLOQUE 2 MEJORADO"""

# BLOQUE 2B CON MEJORAS
# Imputaci√≥n robusta por provincia (en modo mediana): primero rellena por mediana dentro de cada provincia y luego, si queda algo, con mediana global.
# Recorte de outliers con un MAD‚Äëclip por columna num√©rica (suave, robusto, no lineal).
# Limpieza de columnas sin informaci√≥n: elimina features con varianza cero.
# Estabilidad y eficiencia: orden determinista de features y dtype float32 para ahorrar memoria en X_seq.

# ------------------------------
# ‚öôÔ∏è CONFIGURACI√ìN
# ------------------------------
INTERPOLAR_DATOS = False   # ‚¨ÖÔ∏è usa mediana en vez de interpolaci√≥n si False
LOOKBACK_L = 4
HORIZON_H = 1

# ‚úÖ Peque√±os ajustes (on/off)
CLIP_OUTLIERS = True       # Mejora 2: recorte robusto de outliers (MAD clip)
DROP_ZERO_VAR = True       # Mejora 3: eliminar columnas sin varianza
CAST_FLOAT32 = True        # Mejora 4: usar float32 para X (memoria/velocidad)
# ------------------------------

import re
import numpy as np
import pandas as pd

# 1) Normalizar columnas
to_snake = lambda s: re.sub(r"_+","_", re.sub(r"[^\w]+","_", str(s).strip())).strip("_").lower()
df2 = df2.copy()
df2.columns = [to_snake(c) for c in df2.columns]

# 2) Variables clave
COL_YEAR      = 'year'
COL_PROV      = 'province'
COL_POLL_NAME = 'air_pollutant'
COL_POLL_VAL  = 'air_pollution_level'
COL_LAT       = 'latitude'
COL_LON       = 'longitude'
COL_TARGET    = 'respiratory_deaths_per_100k'
COL_PIB       = 'pib'
COL_POP       = 'population'

# 3) Pivot de contaminantes
pivot = (df2.pivot_table(index=[COL_PROV, COL_YEAR],
                         columns=COL_POLL_NAME,
                         values=COL_POLL_VAL,
                         aggfunc='mean')
           .reset_index())
pivot.columns = [to_snake(c if isinstance(c, str) else c[-1]) for c in pivot.columns]

# 4) Agregados por provincia-a√±o
agg = (df2.groupby([COL_PROV, COL_YEAR], as_index=False)
         [[COL_LAT, COL_LON, COL_TARGET, COL_PIB, COL_POP]].mean())
agg.columns = [to_snake(c) for c in agg.columns]

# 5) Asegurar todas las combinaciones de provincia-a√±o
prov_col   = to_snake(COL_PROV)
year_col   = to_snake(COL_YEAR)
lat_col    = to_snake(COL_LAT)
lon_col    = to_snake(COL_LON)
target_col = to_snake(COL_TARGET)
pib_col    = to_snake(COL_PIB)
pop_col    = to_snake(COL_POP)

all_years = np.arange(df2[COL_YEAR].min(), df2[COL_YEAR].max() + 1)
all_provs = df2[COL_PROV].unique()
full_index = pd.MultiIndex.from_product([all_provs, all_years], names=[COL_PROV, COL_YEAR])

# Unir pivot y agregados
df_pa = (
    pivot.merge(agg, on=[prov_col, year_col], how='outer')
         .set_index([prov_col, year_col])
         .reindex(full_index)
         .reset_index()
         .rename(columns={COL_PROV: prov_col, COL_YEAR: year_col})
)

# 6) Interpolaci√≥n o mediana (Mejora 1: mediana por provincia antes que global)
if INTERPOLAR_DATOS:
    df_pa = (
        df_pa
        .groupby(prov_col, group_keys=False)
        .apply(lambda g: g.sort_values(year_col).interpolate(method="linear", limit_direction="both"))
        .reset_index(drop=True)
    )
else:
    # Primero mediana dentro de cada provincia
    num_cols_all = df_pa.select_dtypes(include='number').columns
    df_pa[num_cols_all] = (
        df_pa.groupby(prov_col)[num_cols_all]
             .transform(lambda s: s.fillna(s.median()))
    )
    # Fallback con mediana global si a√∫n quedase algo
    df_pa = df_pa.fillna(df_pa.median(numeric_only=True))

# 7) Relleno de seguridad y recorte de outliers (Mejora 2)
numeric_cols = df_pa.select_dtypes(include='number').columns.difference([target_col])
df_pa[numeric_cols] = df_pa[numeric_cols].replace([np.inf, -np.inf], np.nan)
df_pa[numeric_cols] = df_pa[numeric_cols].fillna(df_pa[numeric_cols].median())

if CLIP_OUTLIERS and len(numeric_cols) > 0:
    # MAD clip por columna (robusto)
    def _mad_clip(s, k=5.0):
        med = np.median(s.values)
        mad = np.median(np.abs(s.values - med))
        if mad == 0 or not np.isfinite(mad):
            return s
        # sigma ‚âà 1.4826 * MAD
        sigma = 1.4826 * mad
        lo, hi = med - k*sigma, med + k*sigma
        return s.clip(lower=lo, upper=hi)
    df_pa[numeric_cols] = df_pa[numeric_cols].apply(_mad_clip)

# 8) Construcci√≥n de features
cont_cols = [c for c in df_pa.columns if c not in [prov_col, year_col, target_col, lat_col, lon_col, pib_col, pop_col]]
features = cont_cols + [lat_col, lon_col, pib_col, pop_col]

# Codificaci√≥n c√≠clica del a√±o
ymin, ymax = int(df_pa[year_col].min()), int(df_pa[year_col].max())
year_norm = (df_pa[year_col] - ymin) / max(1, (ymax - ymin))
df_pa["year_sin"] = np.sin(2*np.pi*year_norm)
df_pa["year_cos"] = np.cos(2*np.pi*year_norm)
features += ['year_sin', 'year_cos']

# Mejora 3: eliminar columnas sin varianza (constantes)
if DROP_ZERO_VAR:
    zero_var = [c for c in features if df_pa[c].nunique(dropna=False) <= 1]
    if zero_var:
        features = [c for c in features if c not in zero_var]

# Mejora 4: orden determinista de features y casteo a float32
features = sorted(features)

# 9) Construcci√≥n de secuencias LSTM
def build_sequences(df_pa, feats, target, groupcol, year_col, L=3, H=1, cast_float32=True):
    X_list, y_list, g_list = [], [], []
    for prov, g in df_pa.groupby(groupcol):
        g = g.sort_values(year_col)
        Xg = g[feats].values
        yg = g[target].values
        T  = len(g)
        for t in range(L, T - (H - 1)):
            if not np.isnan(yg[t + (H - 1)]):
                X_list.append(Xg[t-L:t])
                y_list.append(yg[t + (H - 1)])
                g_list.append(prov)
    if len(X_list) == 0:
        raise ValueError("No se generaron secuencias. Revisa L/H o datos disponibles.")
    X = np.stack(X_list)
    if cast_float32:
        X = X.astype(np.float32, copy=False)
    return X, np.array(y_list), np.array(g_list)

X_seq, y_vec, groups = build_sequences(df_pa, features, target_col, prov_col, year_col,
                                       L=LOOKBACK_L, H=HORIZON_H, cast_float32=CAST_FLOAT32)

print(f"‚úÖ Secuencias listas: X ‚Üí {X_seq.shape} (dtype={X_seq.dtype}), y ‚Üí {y_vec.shape}, provincias ‚Üí {np.unique(groups).size}")

# df_pa.to_csv("df_pa_provincia_a√±o.csv", index=False)

"""# OPTIMIZACI√ìN BLOQUE 3 LSTM STACKED"""

# ===============================================================
# BLOQUE 3 ‚Äî Stacked RNN (LSTM/GRU) + HPO + Estabilizaci√≥n num√©rica
# ===============================================================

import os, time, math, numpy as np, pandas as pd, tensorflow as tf
from sklearn.model_selection import GroupKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Masking, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# -------------------------------
# ‚öôÔ∏è CONFIGURACI√ìN EDITABLE
# -------------------------------
SEED = 42
np.random.seed(SEED); tf.random.set_seed(SEED)

# I/O
MODEL_DIR = "LSTM-OPT-15"        # üëâ carpeta de salida
os.makedirs(MODEL_DIR, exist_ok=True)

# Cross-validation
N_SPLITS_FINAL = 7               # üëâ n¬∫ folds para entrenamiento final
N_SPLITS_HPO   = 7               # üëâ n¬∫ folds que usa el HPO (baja a 3‚Äì5 si quieres acelerar)
PROV_ALWAYS_TRAIN = []           # üëâ provincias que NO deben caer en test (matchea con `groups`)

# Entrenamiento final
FINAL_EPOCHS     = 30            # üëâ √©pocas tope en el entrenamiento final
PATIENCE_FINAL   = 10            # üëâ paciencia de EarlyStopping final

# Estabilidad
TARGET_TRANSFORM = "zscore"      # üëâ 'zscore' | 'log1p' | None
USE_HUBER        = True          # üëâ usar p√©rdida Huber en lugar de MSE
HUBER_DELTA      = 1.0
CLIPNORM         = 1.0           # üëâ clipping de gradiente por norma
USE_MIXED_PREC   = True         # üëâ mixed precision (True requiere GPU moderna)
JIT_COMPILE      = False         # üëâ compilar con XLA (puede acelerar)
USE_TFDATA       = True         # üëâ usar tf.data (m√°s rendimiento) en lugar de arrays
USE_GRU          = False         # üëâ usar GRU (‚âà30‚Äì40% m√°s r√°pida) en vez de LSTM

# ReduceLROnPlateau
USE_REDUCE_LR    = True
RLROP_FACTOR     = 0.5
RLROP_PATIENCE   = 3
RLROP_MIN_LR     = 1e-5

# ====== Guardado de predicciones y TOP-K errores ======
SAVE_PREDICTIONS = True
TOPK_ERRORS      = 50

# ====== Ponderaci√≥n por provincia en TRAIN ======
WEIGHT_BY_PROVINCE = True     # ‚Üê activa/desactiva
WEIGHT_POWER       = 1.0      # peso = 1 / (count ** power)

# HPO (random search)
N_TRIALS        = 15
MAX_EPOCHS_TUNE = 40
PATIENCE_TUNE   = 8
VAL_FRAC        = 0.10
METRIC_PRIMARY  = "r2"         # 'r2' | 'rmse' | 'mae'

SEARCH_SPACE = {
    "UNITS_1":        [32, 48, 64, 96],
    "UNITS_2":        [16, 24, 32, 48],
    "DROPOUT":        [0.0, 0.1, 0.2, 0.3],
    "LR":             [1e-3, 5e-4, 3e-4, 1e-4],
    "BS":             [24, 32, 40, 48, 64, 72],
    "BIDIRECTIONAL":  [True, False],
    "SCALER_TYPE":    ["robust", "standard", "minmax"],
}

# -------------------------------
# üîß Utilidades
# -------------------------------
if USE_MIXED_PREC:
    try:
        tf.keras.mixed_precision.set_global_policy("mixed_float16")
        print("‚úì mixed_precision activada")
    except Exception as e:
        print("‚ö†Ô∏è No se pudo activar mixed_precision:", e)

def get_scaler(tipo):
    if tipo == "standard": return StandardScaler()
    if tipo == "minmax":   return MinMaxScaler()
    if tipo == "robust":   return RobustScaler()
    raise ValueError("SCALER_TYPE debe ser 'standard', 'minmax' o 'robust'")

class IdentityScaler:
    def fit(self, y): return self
    def transform(self, y): return y
    def inverse_transform(self, y): return y

class Log1pScaler:
    def fit(self, y): return self
    def transform(self, y): return np.log1p(y)
    def inverse_transform(self, y): return np.expm1(y)

def make_y_scaler(kind):
    if kind == "zscore": return StandardScaler()
    if kind == "log1p":  return Log1pScaler()
    return IdentityScaler()

def sample_config(rng):
    cfg = {k: rng.choice(v) for k, v in SEARCH_SPACE.items()}
    if cfg["BIDIRECTIONAL"]:  # recorte ligero de unidades si BiRNN
        cfg["UNITS_1"] = int(max(32, round(cfg["UNITS_1"]*0.75/8)*8))
        cfg["UNITS_2"] = int(max(16, round(cfg["UNITS_2"]*0.75/8)*8))
    return cfg

def make_model(input_shape, cfg):
    rnn1 = (GRU if USE_GRU else LSTM)(cfg["UNITS_1"], return_sequences=True)
    rnn2 = (GRU if USE_GRU else LSTM)(cfg["UNITS_2"], return_sequences=False)
    if cfg["BIDIRECTIONAL"]:
        rnn1 = Bidirectional(rnn1)
        rnn2 = Bidirectional(rnn2)
    layers = [
        Masking(mask_value=0.0, input_shape=input_shape),
        rnn1, Dropout(cfg["DROPOUT"]),
        rnn2, Dropout(cfg["DROPOUT"]),
        Dense(1, dtype="float32" if USE_MIXED_PREC else None)
    ]
    model = Sequential(layers)
    opt = Adam(learning_rate=cfg["LR"], clipnorm=CLIPNORM)
    loss_fn = tf.keras.losses.Huber(delta=HUBER_DELTA) if USE_HUBER else "mse"
    try:
        model.compile(optimizer=opt, loss=loss_fn, metrics=["mae"], jit_compile=JIT_COMPILE)
    except TypeError:
        model.compile(optimizer=opt, loss=loss_fn, metrics=["mae"])
    return model

def make_ds(X, y, bs, shuffle=False):
    ds = tf.data.Dataset.from_tensor_slices((X, y))
    if shuffle: ds = ds.shuffle(min(len(X), 10000), reshuffle_each_iteration=True)
    return ds.batch(bs).prefetch(tf.data.AUTOTUNE)

def eval_metrics(y_true, y_pred):
    rmse = mean_squared_error(y_true, y_pred) ** 0.5
    mae  = mean_absolute_error(y_true, y_pred)
    r2   = r2_score(y_true, y_pred)
    return rmse, mae, r2

def make_province_weights(group_ids, power=1.0, normalize=True):
    """Devuelve sample_weight inverso a la frecuencia por provincia para TRAIN."""
    vals, counts = np.unique(group_ids, return_counts=True)
    invfreq = {p: 1.0 / (c ** power) for p, c in zip(vals, counts)}
    w = np.array([invfreq[p] for p in group_ids], dtype="float32")
    if normalize:
        w *= (len(w) / w.sum())
    return w

# -------------------------------
# üîé HPO (Random Search)
# -------------------------------
rng = np.random.RandomState(SEED)
gkf_hpo = GroupKFold(n_splits=N_SPLITS_HPO)
splits_hpo = list(gkf_hpo.split(X_seq, y_vec, groups=groups))

rows = []
print("\nüîé Buscando hiperpar√°metros (Random Search):")
t0 = time.time()
for t in range(1, N_TRIALS+1):
    cfg = sample_config(rng)

    # listas de m√©tricas en VALIDACI√ìN (por fold)
    r_list, m_list, r2_list = [], [], []

    for tr_idx, te_idx in splits_hpo:
        # solo usamos train->(train/val) para HPO
        X_tr, y_tr = X_seq[tr_idx], y_vec[tr_idx]
        n = len(X_tr)
        if n < 4:
            continue
        val_split = max(1, int((1-VAL_FRAC)*n))
        val_split = min(val_split, n-1)

        X_train, X_val = X_tr[:val_split], X_tr[val_split:]
        y_train, y_val = y_tr[:val_split], y_tr[val_split:]

        # Escalado de X
        xscaler = get_scaler(cfg["SCALER_TYPE"]).fit(X_train.reshape(-1, X_train.shape[2]))
        def scX(X): return xscaler.transform(X.reshape(-1, X.shape[2])).reshape(X.shape)
        X_train_sc, X_val_sc = scX(X_train), scX(X_val)

        # Escalado del objetivo
        yscaler = make_y_scaler(TARGET_TRANSFORM)
        yscaler.fit(y_train.reshape(-1,1))
        y_train_t = yscaler.transform(y_train.reshape(-1,1)).ravel()
        y_val_t   = yscaler.transform(y_val.reshape(-1,1)).ravel()

        # ===== Pesos por provincia en TRAIN (opcional) =====
        tr_idx_train = tr_idx[:val_split]                 # √≠ndices reales de TRAIN
        prov_train   = groups[tr_idx_train]
        w_train = make_province_weights(prov_train, WEIGHT_POWER) if WEIGHT_BY_PROVINCE else None

        # Modelo + callbacks
        tf.keras.backend.clear_session()
        model = make_model((X_seq.shape[1], X_seq.shape[2]), cfg)
        callbacks = [EarlyStopping(monitor="val_loss", patience=PATIENCE_TUNE, restore_best_weights=True)]
        if USE_REDUCE_LR:
            callbacks.append(ReduceLROnPlateau(monitor="val_loss",
                                               factor=RLROP_FACTOR, patience=RLROP_PATIENCE,
                                               min_lr=RLROP_MIN_LR, verbose=0))

        # Entrenar
        if USE_TFDATA:
            if WEIGHT_BY_PROVINCE and w_train is not None:
                train_ds = tf.data.Dataset.from_tensor_slices((X_train_sc, y_train_t, w_train)).batch(cfg["BS"]).prefetch(tf.data.AUTOTUNE)
                val_ds   = tf.data.Dataset.from_tensor_slices((X_val_sc,   y_val_t)).batch(cfg["BS"]).prefetch(tf.data.AUTOTUNE)
                model.fit(train_ds, validation_data=val_ds, epochs=MAX_EPOCHS_TUNE, verbose=0, callbacks=callbacks)
            else:
                train_ds = make_ds(X_train_sc, y_train_t, cfg["BS"], shuffle=True)
                val_ds   = make_ds(X_val_sc,   y_val_t,   cfg["BS"], shuffle=False)
                model.fit(train_ds, validation_data=val_ds, epochs=MAX_EPOCHS_TUNE, verbose=0, callbacks=callbacks)
            y_pred_t = model.predict(val_ds, verbose=0).ravel()
        else:
            model.fit(X_train_sc, y_train_t,
                      validation_data=(X_val_sc, y_val_t),
                      epochs=MAX_EPOCHS_TUNE, batch_size=cfg["BS"], verbose=0,
                      callbacks=callbacks,
                      sample_weight=(w_train if WEIGHT_BY_PROVINCE else None))
            y_pred_t = model.predict(X_val_sc, verbose=0).ravel()

        # Desescalar y m√©tricas en escala original
        y_pred = yscaler.inverse_transform(y_pred_t.reshape(-1,1)).ravel()
        rmse, mae, r2 = eval_metrics(y_val, y_pred)
        r_list.append(rmse); m_list.append(mae); r2_list.append(r2)

    if r2_list:
        rm, mm, rr = float(np.mean(r_list)), float(np.mean(m_list)), float(np.mean(r2_list))
    else:
        rm, mm, rr = np.inf, np.inf, -np.inf

    rows.append({"trial": t, **cfg, "rmse_val": rm, "mae_val": mm, "r2_val": rr})
    print(f"Trial {t:02d} | {cfg} -> RMSE(val) {rm:.3f}  MAE(val) {mm:.3f}  R¬≤(val) {rr:.3f}")

# Resultados HPO -> CSVs
hpo_df = pd.DataFrame(rows).sort_values(
    "r2_val" if METRIC_PRIMARY=="r2" else ("rmse_val" if METRIC_PRIMARY=="rmse" else "mae_val"),
    ascending=(METRIC_PRIMARY!="r2")
).reset_index(drop=True)
hpo_df.to_csv(os.path.join(MODEL_DIR, "hpo_trials.csv"), index=False)

best_cfg = hpo_df.iloc[0][["UNITS_1","UNITS_2","DROPOUT","LR","BS","BIDIRECTIONAL","SCALER_TYPE"]].to_dict()
best_scores = hpo_df.iloc[0][["rmse_val","mae_val","r2_val"]].to_dict()
best_cfg_df = pd.DataFrame([{**best_cfg, **best_scores}])
best_cfg_df.to_csv(os.path.join(MODEL_DIR, "hpo_best_config.csv"), index=False)

print(f"\n‚úÖ HPO completado en {time.time()-t0:.1f}s con {len(hpo_df)} configs v√°lidas")
print("\nü•á Mejor configuraci√≥n encontrada (seg√∫n", METRIC_PRIMARY.upper(), "en validaci√≥n):")
print(best_cfg)
print(f"Scores val ‚Üí RMSE: {best_scores['rmse_val']:.3f} | MAE: {best_scores['mae_val']:.3f} | R¬≤: {best_scores['r2_val']:.3f}")

# Aplicar mejor config al entrenamiento final
UNITS_1       = int(best_cfg["UNITS_1"])
UNITS_2       = int(best_cfg["UNITS_2"])
DROPOUT       = float(best_cfg["DROPOUT"])
LR            = float(best_cfg["LR"])
BS            = int(best_cfg["BS"])
BIDIRECTIONAL = bool(best_cfg["BIDIRECTIONAL"])
SCALER_TYPE   = str(best_cfg["SCALER_TYPE"])

# -------------------------------
# üß† Entrenamiento final por GroupKFold (con y escalada y desescalado en test)
# -------------------------------
gkf_final = GroupKFold(n_splits=N_SPLITS_FINAL)
splits_final = list(gkf_final.split(X_seq, y_vec, groups=groups))

results = []
fold_history = []
models_dir = os.path.join(MODEL_DIR, "best_models")
os.makedirs(models_dir, exist_ok=True)

for fold, (tr_idx, te_idx) in enumerate(splits_final, start=1):
    print(f"\n===== Fold {fold} =====")

    # evitar provincias vetadas en test
    mask_keep = ~np.isin(groups[te_idx], PROV_ALWAYS_TRAIN)
    te_idx = te_idx[mask_keep]

    X_tr, X_te = X_seq[tr_idx], X_seq[te_idx]
    y_tr, y_te = y_vec[tr_idx], y_vec[te_idx]
    prov_te    = groups[te_idx]
    prov_names = ", ".join(sorted(np.unique(prov_te)))

    # split interno train/val (√∫ltimo 10% para val)
    n = len(X_tr)
    val_split = max(1, int(0.9*n))
    val_split = min(val_split, n-1)
    X_train, X_val = X_tr[:val_split], X_tr[val_split:]
    y_train, y_val = y_tr[:val_split], y_tr[val_split:]

    # Escalado X
    xscaler = get_scaler(SCALER_TYPE).fit(X_train.reshape(-1, X_train.shape[2]))
    def scX(X): return xscaler.transform(X.reshape(-1, X.shape[2])).reshape(X.shape)
    X_train_sc, X_val_sc, X_te_sc = scX(X_train), scX(X_val), scX(X_te)

    # Escalado objetivo (fit SOLO con y_train)
    yscaler = make_y_scaler(TARGET_TRANSFORM)
    yscaler.fit(y_train.reshape(-1,1))
    y_train_t = yscaler.transform(y_train.reshape(-1,1)).ravel()
    y_val_t   = yscaler.transform(y_val.reshape(-1,1)).ravel()

    # ===== Pesos por provincia en TRAIN (opcional) =====
    tr_idx_train = tr_idx[:val_split]     # √≠ndices reales de TRAIN
    prov_train   = groups[tr_idx_train]
    w_train = make_province_weights(prov_train, WEIGHT_POWER) if WEIGHT_BY_PROVINCE else None

    # Modelo + callbacks
    tf.keras.backend.clear_session()
    cfg_final = {"UNITS_1":UNITS_1,"UNITS_2":UNITS_2,"DROPOUT":DROPOUT,"LR":LR,"BS":BS,
                 "BIDIRECTIONAL":BIDIRECTIONAL,"SCALER_TYPE":SCALER_TYPE}
    model = make_model((X_seq.shape[1], X_seq.shape[2]), cfg_final)

    callbacks = [EarlyStopping(monitor="val_loss", patience=PATIENCE_FINAL, restore_best_weights=True)]
    if USE_REDUCE_LR:
        callbacks.append(ReduceLROnPlateau(monitor="val_loss",
                                           factor=RLROP_FACTOR, patience=RLROP_PATIENCE,
                                           min_lr=RLROP_MIN_LR, verbose=0))

    # Entrenar
    if USE_TFDATA:
        if WEIGHT_BY_PROVINCE and w_train is not None:
            train_ds = tf.data.Dataset.from_tensor_slices((X_train_sc, y_train_t, w_train)).batch(BS).prefetch(tf.data.AUTOTUNE)
            val_ds   = tf.data.Dataset.from_tensor_slices((X_val_sc,   y_val_t)).batch(BS).prefetch(tf.data.AUTOTUNE)
            test_ds  = tf.data.Dataset.from_tensor_slices((X_te_sc, np.zeros_like(y_te))).batch(BS).prefetch(tf.data.AUTOTUNE)
            history = model.fit(train_ds, validation_data=val_ds, epochs=FINAL_EPOCHS, verbose=0, callbacks=callbacks)
            fold_history.append(pd.DataFrame(history.history).assign(fold=fold))
        else:
            train_ds = make_ds(X_train_sc, y_train_t, BS, shuffle=True)
            val_ds   = make_ds(X_val_sc,   y_val_t,   BS, shuffle=False)
            test_ds  = tf.data.Dataset.from_tensor_slices((X_te_sc, np.zeros_like(y_te))).batch(BS).prefetch(tf.data.AUTOTUNE)
            history = model.fit(train_ds, validation_data=val_ds, epochs=FINAL_EPOCHS, verbose=0, callbacks=callbacks)
            fold_history.append(pd.DataFrame(history.history).assign(fold=fold))
        y_pred_t = model.predict(test_ds, verbose=0).ravel()
    else:
        history = model.fit(X_train_sc, y_train_t,
                        validation_data=(X_val_sc, y_val_t),
                        epochs=FINAL_EPOCHS, batch_size=BS, verbose=0,
                        callbacks=callbacks,
                        sample_weight=(w_train if WEIGHT_BY_PROVINCE else None))
        fold_history.append(pd.DataFrame(history.history).assign(fold=fold))
        y_pred_t = model.predict(X_te_sc, verbose=0).ravel()

    # Desescalar y evaluar en TEST (escala original)
    y_pred = yscaler.inverse_transform(y_pred_t.reshape(-1,1)).ravel()
    rmse, mae, r2 = eval_metrics(y_te, y_pred)

    # Baseline para contexto: predecir media de y_train
    y_base = np.full_like(y_te, np.mean(y_train))
    r2_base = r2_score(y_te, y_base)

    # Guardar modelo
    model_name = f"best_fold{fold}_rmse_{rmse:.3f}_r2_{r2:.3f}.keras"
    model.save(os.path.join(models_dir, model_name))

    # ===== (Punto 2) Guardar predicciones y TOP-K errores por fold =====
    if SAVE_PREDICTIONS:
        pred_df = pd.DataFrame({
            "sample_id": te_idx,
            "province":  prov_te,
            "y_true":    y_te,
            "y_pred":    y_pred,
            "abs_err":   np.abs(y_te - y_pred),
            "sq_err":    (y_te - y_pred)**2
        })
        fold_prefix = f"fold{fold}"
        pred_df.to_csv(os.path.join(models_dir, f"predicciones_{fold_prefix}.csv"), index=False)
        topk_df = pred_df.sort_values("abs_err", ascending=False).head(TOPK_ERRORS)
        topk_df.to_csv(os.path.join(models_dir, f"top_errores_{fold_prefix}_top{TOPK_ERRORS}.csv"), index=False)

    results.append({"fold": fold, "rmse": rmse, "mae": mae, "r2": r2,
                    "r2_baseline": r2_base,
                    "n_provincias_test": int(np.unique(prov_te).size),
                    "provincias_test": prov_names,
                    "model": model_name})
    print(f"RMSE={rmse:.3f} | MAE={mae:.3f} | R¬≤={r2:.3f} (baseline {r2_base:.3f}) ‚Üí {model_name}")

# Resultados finales
results_df = pd.DataFrame(results).sort_values("fold").reset_index(drop=True)
results_df.to_csv(os.path.join(models_dir, "eval_best_models.csv"), index=False)
print("\n=== Resultados globales (final) ===")
display(results_df.round(3))

model.summary()

# BLOQUE 3 EXTRa - VIsualizaci√≥n
# lee fold_history qye est√° en memoria

# ========= BLOQUE 3 EXTRA ‚Äì Visualizaci√≥n =========
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os

# toggles
SMOOTH_K   = 3          # tama√±o de ventana para suavizar (0/1 = sin suavizado)
SAVE_FIG   = True       # guardar PNG en MODEL_DIR
FIGSIZE    = (17, 6)    # tama√±o de la figura

# 1) Unir historiales
if not isinstance(fold_history, list) or len(fold_history) == 0:
    raise RuntimeError("fold_history est√° vac√≠o. Aseg√∫rate de hacer append del history por fold.")

df_hist = pd.concat(fold_history, ignore_index=True)

# A√±adir columna epoch si no existe
if "epoch" not in df_hist.columns:
    df_hist["epoch"] = df_hist.groupby("fold").cumcount()

# helper de suavizado
def smooth(series, k=SMOOTH_K):
    if k and k > 1:
        return series.rolling(k, min_periods=1, center=True).mean()
    return series

# 2) Crear figura
fig, axs = plt.subplots(1, 2, figsize=FIGSIZE)

# --------- LOSS ----------
for fold in sorted(df_hist['fold'].unique()):
    fdf = df_hist[df_hist['fold'] == fold].copy()
    epochs = fdf["epoch"].to_numpy()
    color  = f"C{(int(fold)-1) % 10}"

    axs[0].plot(epochs, smooth(fdf["loss"]), label=f"Fold {fold} ¬∑ train", color=color)
    if "val_loss" in fdf.columns:
        axs[0].plot(epochs, smooth(fdf["val_loss"]), ls="--", label=f"Fold {fold} ¬∑ val", color=color)
        # marcar mejor √©poca (m√≠nimo val_loss)
        best_i = fdf["val_loss"].idxmin()
        best_ep = int(df_hist.loc[best_i, "epoch"])
        best_val = float(df_hist.loc[best_i, "val_loss"])
        axs[0].scatter([best_ep], [best_val], color=color, s=35, zorder=5)
        axs[0].axvline(best_ep, color=color, alpha=0.15)

axs[0].set_title("Loss por √©poca")
axs[0].set_xlabel("√âpoca"); axs[0].set_ylabel("Loss")
axs[0].grid(True)
axs[0].legend(loc="center left", bbox_to_anchor=(1, 0.5), title="Curvas")

# --------- MAE ----------
for fold in sorted(df_hist['fold'].unique()):
    fdf = df_hist[df_hist['fold'] == fold].copy()
    epochs = fdf["epoch"].to_numpy()
    color  = f"C{(int(fold)-1) % 10}"

    if "mae" in fdf.columns:
        axs[1].plot(epochs, smooth(fdf["mae"]), label=f"Fold {fold} ¬∑ train", color=color)
    if "val_mae" in fdf.columns:
        axs[1].plot(epochs, smooth(fdf["val_mae"]), ls="--", label=f"Fold {fold} ¬∑ val", color=color)

axs[1].set_title("MAE por √©poca")
axs[1].set_xlabel("√âpoca"); axs[1].set_ylabel("MAE (escala transformada)")
axs[1].grid(True)
axs[1].legend(loc="center left", bbox_to_anchor=(1, 0.5), title="Curvas")

plt.suptitle("Curvas de entrenamiento (l√≠nea cont√≠nua) y validaci√≥n (l√≠nea discontinua) por fold")
plt.tight_layout()

if SAVE_FIG:
    out = os.path.join(MODEL_DIR, "curvas_entrenamiento.png")
    plt.savefig(out, dpi=150, bbox_inches="tight")
    print("Figura guardada en:", out)

plt.show()
# =============================================================

# from google.colab import drive
# drive.mount('/content/drive')

# !cp -r /content/LSTM-STACKED-ROB-08 /content/drive/MyDrive/

"""# Evaluaci√≥n con el mejor Fold"""

rut0 =  "./LSTM-OPT-14/best_models/eval_best_models.csv"
dfr0 = pd.read_csv(rut0)
print(dfr0)

# =========================================================
# BLOQUE 4 ‚Äì Visualizaci√≥n real vs. predicho (por fold)
# =========================================================
import os, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
from tensorflow.keras.models import load_model
from sklearn.model_selection import GroupKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# -------------------------
# ‚öôÔ∏è CONFIG editable
# -------------------------
MODEL_DIR        = "LSTM-OPT-15"   # ‚Üê carpeta usada en BLOQUE 3
FOLD_TO_PLOT     = 1               # ‚Üê fold que quieres visualizar (1..N_SPLITS_FINAL)
AUTO_PICK_MODEL  = False            # ‚Üê True: lee eval_best_models.csv y coge el modelo del fold
MODEL_FILE       = "./LSTM-OPT-15/best_models/best_fold1_rmse_18.255_r2_0.618.keras"            # ‚Üê la ruta al .keras si AUTO_PICK_MODEL=False
TARGET_TRANSFORM = "zscore"        # ‚Üê DEBE coincidir con BLOQUE 3: 'zscore' | 'log1p' | None
SCALER_TYPE      = None            # ‚Üê None: lee hpo_best_config.csv; o fuerza: 'robust'/'standard'/'minmax'
PROV_ALWAYS_TRAIN = []             # ‚Üê igual que en BLOQUE 3
SAVE_PNG         = True

# este archivo es el modelo entrenado en el train del fold 1 (con su 10% de val interna)
# los n√∫meros en el nombre(rmse y r^2) son el rendimiento en el test del fold 1
# El fold es una partici√≥n de la validaci√≥n cruzada, que se produce en train.


# -------------------------
# Utilidades (id√©nticas a BLOQUE 3)
# -------------------------
def get_scaler(tipo):
    if tipo == "standard": return StandardScaler()
    if tipo == "minmax":   return MinMaxScaler()
    if tipo == "robust":   return RobustScaler()
    raise ValueError("SCALER_TYPE debe ser 'standard', 'minmax' o 'robust'")

class IdentityScaler:
    def fit(self, y): return self
    def transform(self, y): return y
    def inverse_transform(self, y): return y

class Log1pScaler:
    def fit(self, y): return self
    def transform(self, y): return np.log1p(y)
    def inverse_transform(self, y): return np.expm1(y)

def make_y_scaler(kind):
    if kind == "zscore": return StandardScaler()
    if kind == "log1p":  return Log1pScaler()
    return IdentityScaler()

# -------------------------
# 1) Leer config y modelo del fold
# -------------------------
# SCALER_TYPE del HPO si no lo forzas
if SCALER_TYPE is None:
    cfg_csv = os.path.join(MODEL_DIR, "hpo_best_config.csv")
    if not os.path.exists(cfg_csv):
        raise FileNotFoundError(f"No encuentro {cfg_csv}. Ejecuta el BLOQUE 3 primero.")
    SCALER_TYPE = pd.read_csv(cfg_csv).iloc[0]["SCALER_TYPE"]
print(f"Usando SCALER_TYPE='{SCALER_TYPE}' y TARGET_TRANSFORM='{TARGET_TRANSFORM}'")

# Modelo del fold
if AUTO_PICK_MODEL:
    eval_csv = os.path.join(MODEL_DIR, "best_models", "eval_best_models.csv")
    if not os.path.exists(eval_csv):
        raise FileNotFoundError(f"No encuentro {eval_csv}. Ejecuta el BLOQUE 3 (entrenamiento final).")
    row = pd.read_csv(eval_csv)
    if FOLD_TO_PLOT < 1 or FOLD_TO_PLOT > row["fold"].max():
        raise ValueError(f"FOLD_TO_PLOT debe estar entre 1 y {int(row['fold'].max())}")
    model_name = row.set_index("fold").loc[FOLD_TO_PLOT, "model"]
    MODEL_FILE = os.path.join(MODEL_DIR, "best_models", model_name)
else:
    if MODEL_FILE is None:
        raise ValueError("Si AUTO_PICK_MODEL=False debes indicar MODEL_FILE.")

if not os.path.exists(MODEL_FILE):
    raise FileNotFoundError(f"No se encontr√≥ el modelo: {MODEL_FILE}")
print("‚úÖ Modelo a usar:", MODEL_FILE)

# -------------------------
# 2) Reconstruir el fold y los escaladores (como en entrenamiento)
# -------------------------
gkf = GroupKFold(n_splits=max(FOLD_TO_PLOT, 7))  # seguro que tenemos suficientes splits
splits = list(gkf.split(X_seq, y_vec, groups=groups))
tr_idx, te_idx = splits[FOLD_TO_PLOT - 1]

# quitar provincias vetadas en test
mask_keep = ~np.isin(groups[te_idx], PROV_ALWAYS_TRAIN)
te_idx = te_idx[mask_keep]

X_tr, X_te = X_seq[tr_idx], X_seq[te_idx]
y_tr, y_te = y_vec[tr_idx], y_vec[te_idx]
prov_te    = groups[te_idx]

# Escalar X con el mismo tipo y "fit" SOLO en X_train (como en BLOQUE 3)
xscaler = get_scaler(SCALER_TYPE).fit(X_tr.reshape(-1, X_tr.shape[2]))
X_te_sc = xscaler.transform(X_te.reshape(-1, X_te.shape[2])).reshape(X_te.shape)

# Escalar y con el mismo TARGET_TRANSFORM (fit SOLO con y_train)
yscaler = make_y_scaler(TARGET_TRANSFORM)
yscaler.fit(y_tr.reshape(-1,1))

# Intentar obtener a√±os si existen (years_vec o df_pa/year_col); si no, usa un √≠ndice
try:
    years_te = years_vec[te_idx]         # si tienes un np.array a√±os alineado
except Exception:
    try:
        years_te = df_pa[year_col].values[te_idx]  # si lo ten√≠as en un DataFrame
    except Exception:
        years_te = np.arange(len(y_te))           # fallback: √≠ndice

# -------------------------
# 3) Cargar modelo y predecir (desescalar y_pred)
# -------------------------
# Cargar sin compilar (no necesitamos la loss)
model = load_model(MODEL_FILE, compile=False)
y_pred_t = model.predict(X_te_sc, verbose=0).ravel()
y_pred   = yscaler.inverse_transform(y_pred_t.reshape(-1,1)).ravel()

# -------------------------
# 4) Construir DataFrame de resultados
# -------------------------
df_plot = pd.DataFrame({
    "y_real": y_te,
    "y_pred": y_pred,
    "provincia": prov_te,
    "a√±o": years_te
})
df_plot["abs_error"] = np.abs(df_plot["y_real"] - df_plot["y_pred"])

# -------------------------
# 5) Gr√°ficos
# -------------------------
plots_dir = os.path.join(MODEL_DIR, "plots")
os.makedirs(plots_dir, exist_ok=True)

# A) Dispersi√≥n general (con leyenda)
plt.figure(figsize=(7, 7))
ax = sns.scatterplot(
    data=df_plot,
    x="y_real", y="y_pred",
    hue="provincia",
    hue_order=sorted(df_plot["provincia"].unique()),  # colores consistentes
    alpha=0.8, s=45
)
lims = [df_plot["y_real"].min(), df_plot["y_real"].max()]
plt.plot(lims, lims, "--", color="gray")
plt.title(f"Real vs Predicho ‚Äî Fold {FOLD_TO_PLOT}")
plt.xlabel("Real (muertes/100k)")
plt.ylabel("Predicho (muertes/100k)")
plt.grid(True)

# leyenda fuera del gr√°fico para que quepa
ax.legend(
    title="Provincia",
    bbox_to_anchor=(1.02, 1), loc="upper left",
    borderaxespad=0.0, frameon=True, fontsize=10, ncol=1
)

plt.tight_layout()
if SAVE_PNG:
    out = os.path.join(plots_dir, f"real_vs_pred_fold{FOLD_TO_PLOT}.png")
    plt.savefig(out, dpi=150, bbox_inches="tight")
    print("Figura guardada:", out)
plt.show()

# B) Top-6 mejores/peores provincias por MAE
prov_error = df_plot.groupby("provincia")["abs_error"].mean().sort_values()
mejores = prov_error.head(6); peores = prov_error.tail(6)

def plot_prov_series(title, provs, fname):
    rows = int(np.ceil(len(provs)/3))
    fig, axs = plt.subplots(rows, 3, figsize=(18, 5*rows), sharey=True)
    axs = np.atleast_1d(axs).ravel()
    for ax, prov in zip(axs, provs):
        sub = df_plot[df_plot["provincia"] == prov].sort_values("a√±o")
        anios = sub["a√±o"].values
        ax.plot(anios, sub["y_real"].values, label="Real", marker="o")
        ax.plot(anios, sub["y_pred"].values, label="Predicho", marker="x")
        ax.set_title(f"{prov} (MAE={sub['abs_error'].mean():.2f})")
        ax.set_xlabel("A√±o"); ax.set_ylabel("Muertes/100k")
        try:
            ax.set_xticks(np.arange(int(anios.min()), int(anios.max())+1, 3))
        except Exception:
            pass
        ax.grid(True); ax.legend()
    plt.suptitle(title)
    plt.tight_layout()
    if SAVE_PNG:
        out = os.path.join(plots_dir, fname)
        plt.savefig(out, dpi=150, bbox_inches="tight")
        print("Figura guardada:", out)
    plt.show()

plot_prov_series("Top 6 provincias con menor MAE", mejores.index, f"top_mejores_fold{FOLD_TO_PLOT}.png")
plot_prov_series("Top 6 provincias con mayor MAE", peores.index,  f"top_peores_fold{FOLD_TO_PLOT}.png")

# C) Tabla resumen (mejores/peores)
df_error = pd.concat([
    mejores.to_frame(name="MAE").assign(tipo="Mejor ajuste"),
    peores.to_frame(name="MAE").assign(tipo="Peor ajuste")
]).reset_index().drop_duplicates(subset="provincia").rename(columns={"index":"provincia"})
display(df_error.sort_values(["tipo","MAE"]))

"""# Bloque 5

"""

# ============================================================
# BLOQUE 5 ‚Äì Exportar predicciones y errores a CSV
# ============================================================
import os, numpy as np, pandas as pd
from sklearn.model_selection import GroupKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from tensorflow.keras.models import load_model
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# -------------------------------
# ‚öôÔ∏è CONFIGURACI√ìN
# -------------------------------
MODEL_DIR        = "LSTM-OPT-15"    # carpeta donde est√°n los .keras
FOLD_TO_EXPORT   = 1                        # fold a exportar (1..n_splits)
N_SPLITS         = 7
AUTO_PICK_MODEL  = False                    # True: intenta encontrar el modelo por patr√≥n
MODEL_FILE       = "./LSTM-OPT-15/best_models/best_fold1_rmse_18.255_r2_0.618.keras"  # si AUTO_PICK_MODEL=False
PROV_ALWAYS_TRAIN = [] # si 'groups' son nombres; si son ids, pon los ids
TARGET_TRANSFORM = "zscore"                 # el mismo que usaste al entrenar
SCALER_TYPE      = "robust"               # el mismo que usaste al entrenar ('standard'|'minmax'|'robust')
TOPK_ERRORS      = 50                       # guarda adem√°s el top-K de errores absolutos
OUT_DIR          = MODEL_DIR                # d√≥nde escribir los CSV

# -------------------------------
# Utilidades de escalado
# -------------------------------
def get_scaler(tipo):
    if tipo == "standard": return StandardScaler()
    if tipo == "minmax":   return MinMaxScaler()
    if tipo == "robust":   return RobustScaler()
    raise ValueError("SCALER_TYPE debe ser 'standard', 'minmax' o 'robust'")

class IdentityScaler:
    def fit(self, y): return self
    def transform(self, y): return y
    def inverse_transform(self, y): return y

class Log1pScaler:
    def fit(self, y): return self
    def transform(self, y): return np.log1p(y)
    def inverse_transform(self, y): return np.expm1(y)

def make_y_scaler(kind):
    if kind == "zscore": return StandardScaler()
    if kind == "log1p":  return Log1pScaler()
    return IdentityScaler()

# -------------------------------
# Validaciones y modelo
# -------------------------------
os.makedirs(OUT_DIR, exist_ok=True)
if not (1 <= FOLD_TO_EXPORT <= N_SPLITS):
    raise ValueError(f"FOLD_TO_EXPORT debe estar entre 1 y {N_SPLITS}")

# Si SCALER_TYPE=None, leer del HPO (por si cambi√≥ en tuning)
if SCALER_TYPE is None:
    cfg_csv = os.path.join(MODEL_DIR, "hpo_best_config.csv")
    if not os.path.exists(cfg_csv):
        raise FileNotFoundError(f"No encuentro {cfg_csv}. Ejecuta el BLOQUE 3 primero o fija SCALER_TYPE.")
    SCALER_TYPE = pd.read_csv(cfg_csv).iloc[0]["SCALER_TYPE"]
print(f"Usando SCALER_TYPE='{SCALER_TYPE}' y TARGET_TRANSFORM='{TARGET_TRANSFORM}'")

# Auto-pick del modelo con patr√≥n correcto, si procede
if AUTO_PICK_MODEL:
    import glob
    patt = os.path.join(MODEL_DIR, "best_models", f"best_fold{FOLD_TO_EXPORT}_rmse_*_r2_*.keras")
    cand = sorted(glob.glob(patt))
    if not cand:
        raise FileNotFoundError(f"No se encontr√≥ modelo con patr√≥n: {patt}")
    MODEL_FILE = cand[0]

if not os.path.exists(MODEL_FILE):
    raise FileNotFoundError(f"No se encontr√≥ el modelo: {MODEL_FILE}")

model = load_model(MODEL_FILE, compile=False)
print(f"‚úÖ Modelo cargado: {MODEL_FILE}")

# -------------------------------
# Construir el fold y preprocesar como en BLOQUE 3
# -------------------------------
gkf = GroupKFold(n_splits=N_SPLITS)
splits = list(gkf.split(X_seq, y_vec, groups=groups))
tr_idx, te_idx = splits[FOLD_TO_EXPORT - 1]

# quitar provincias vetadas del test
mask_keep = ~np.isin(groups[te_idx], PROV_ALWAYS_TRAIN)
te_idx = te_idx[mask_keep]

X_tr, X_te = X_seq[tr_idx], X_seq[te_idx]
y_tr, y_te = y_vec[tr_idx], y_vec[te_idx]
prov_te    = groups[te_idx]

# --- split interno como en BLOQUE 3 (√∫ltimo 10% es validaci√≥n) ---
n = len(X_tr)
val_split = max(1, int(0.9 * n))
val_split = min(val_split, n - 1)
X_train, X_val = X_tr[:val_split], X_tr[val_split:]
y_train, y_val = y_tr[:val_split], y_tr[val_split:]

# a√±os (si tienes years_vec o df_pa[year_col])
try:
    years_te = years_vec[te_idx]
except Exception:
    try:
        years_te = df_pa[year_col].values[te_idx]
    except Exception:
        years_te = np.arange(len(y_te))

# Escalado X (fit SOLO en X_train, como en el entrenamiento)
xscaler = get_scaler(SCALER_TYPE).fit(X_train.reshape(-1, X_train.shape[2]))
X_te_sc = xscaler.transform(X_te.reshape(-1, X_te.shape[2])).reshape(X_te.shape)

# Transformaci√≥n de y (fit SOLO en y_train)
yscaler = make_y_scaler(TARGET_TRANSFORM)
yscaler.fit(y_train.reshape(-1, 1))

# -------------------------------
# Predicci√≥n y desescalado
# -------------------------------
y_pred_t = model.predict(X_te_sc, verbose=0).ravel()
y_pred   = yscaler.inverse_transform(y_pred_t.reshape(-1,1)).ravel()

# -------------------------------
# DataFrame de resultados y export
# -------------------------------
df_export = pd.DataFrame({
    "fold": FOLD_TO_EXPORT,
    "provincia": prov_te,
    "a√±o": years_te,
    "y_real": y_te,
    "y_pred": y_pred
})
df_export["abs_error"] = np.abs(df_export["y_real"] - df_export["y_pred"])
df_export["sq_error"]  = (df_export["y_real"] - df_export["y_pred"])**2

# rutas
csv_pred_path    = os.path.join(OUT_DIR, f"predicciones_fold{FOLD_TO_EXPORT}.csv")
csv_summary_path = os.path.join(OUT_DIR, f"mae_provincias_fold{FOLD_TO_EXPORT}.csv")
csv_topk_path    = os.path.join(OUT_DIR, f"top_errores_fold{FOLD_TO_EXPORT}_top{TOPK_ERRORS}.csv")

# guardar predicciones completas
df_export.sort_values(["provincia","a√±o"]).to_csv(csv_pred_path, index=False)
print(f"‚úÖ Predicciones guardadas en: {csv_pred_path}")

# resumen por provincia
df_summary = (df_export.groupby("provincia")
              .agg(real_avg=("y_real","mean"),
                   pred_avg=("y_pred","mean"),
                   mae=("abs_error","mean"),
                   rmse=("sq_error", lambda s: np.sqrt(np.mean(s))),
                   n=("y_real","count"))
              .reset_index()
              .sort_values("mae"))
df_summary.to_csv(csv_summary_path, index=False)
print(f"‚úÖ MAE/RMSE por provincia guardado en: {csv_summary_path}")

# top-K errores absolutos
df_export.sort_values("abs_error", ascending=False).head(TOPK_ERRORS).to_csv(csv_topk_path, index=False)
print(f"‚úÖ Top-{TOPK_ERRORS} errores guardado en: {csv_topk_path}")

# M√©tricas globales del fold
rmse = np.sqrt(np.mean(df_export["sq_error"]))
mae  = df_export["abs_error"].mean()
r2   = r2_score(df_export["y_real"], df_export["y_pred"])
print(f"üìà M√©tricas del fold {FOLD_TO_EXPORT}: RMSE={rmse:.3f}  MAE={mae:.3f}  R¬≤={r2:.3f}")

# -------------------------------
# Sanity check con el CSV de evaluaci√≥n del entrenamiento
# -------------------------------
eval_csv = os.path.join(MODEL_DIR, "best_models", "eval_best_models.csv")
if os.path.exists(eval_csv):
    row = pd.read_csv(eval_csv).set_index("fold").loc[FOLD_TO_EXPORT]
    prov_csv  = set([p.strip() for p in str(row["provincias_test"]).split(",")])
    prov_pred = set(map(str, np.unique(prov_te)))
    print("üîé Coinciden provincias test?:", prov_csv == prov_pred)
    try:
        r2_csv = float(row["r2"])
        print(f"üîé R¬≤ ahora vs CSV: {r2:.3f} vs {r2_csv:.3f}")
    except Exception:
        pass

rut2 =  "./LSTM-OPT-15/mae_provincias_fold1.csv"
dfr2 = pd.read_csv(rut2)
print(dfr2)

rut3 = "./LSTM-OPT-15/predicciones_fold1.csv"
dfr3 = pd.read_csv(rut3)

display(dfr3)

"""# Bloque 6 - diagnostico por provincia"""

# =========================================================
# BLOQUE 6 (MANUAL) ‚Äî Visualizaci√≥n del rendimiento por provincia (por fold)
# =========================================================
# =========================================================
# BLOQUE 6 (MANUAL) ‚Äî Visualizaci√≥n del rendimiento por provincia (por fold)
# Usa CSVs que t√∫ aportas manualmente (rutas abajo).
# - Si hay resumen: lo usa directamente
# - Si NO hay resumen pero S√ç predicciones: lo reconstruye
# - Si hay ambos, usa el resumen y las predicciones solo para m√©tricas globales
# =========================================================

import os, re, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
from unicodedata import normalize as u_norm

# -------- CONFIG MANUAL --------
FOLD_TO_PLOT   = 1
SUMMARY_CSV    = r" "    # ‚Üê ruta al mae_provincias_foldX.csv o deja vac√≠o
PREDICTIONS_CSV= r"./LSTM-OPT-15/best_models/predicciones_fold1.csv"  # o deja esta y usar√° fallbacks
SAVE_PNG       = True
OUT_PNG        = r"./LSTM-OPT-15/plots/prov_mae_y_dispersion_fold1.png"
ORDER_DESC     = True
TOP_LABELS     = 6
# --------------------------------

# ---------- Helpers robustos ----------
def read_csv_auto(path):
    try:
        return pd.read_csv(path)
    except Exception:
        # fallback com√∫n si el CSV usa ';'
        return pd.read_csv(path, sep=';')

def to_key(s: str) -> str:
    s = u_norm("NFKD", str(s)).encode("ascii", "ignore").decode("ascii")
    s = re.sub(r"\s+", "_", s.strip().lower())
    s = re.sub(r"[^a-z0-9_]+", "", s)
    return s

def normalize_pred_columns(df: pd.DataFrame) -> pd.DataFrame:
    # Renombrado robusto (sin√≥nimos habituales)
    keymap = {
        "provincia": "provincia", "province": "provincia",
        "y_real": "y_real", "real": "y_real",
        "real_muertes100k": "y_real", "real_muertes_100k": "y_real",
        "realmuertes100k": "y_real", "yreal": "y_real",
        "y_true": "y_real", # Added mapping for 'y_true'

        "y_pred": "y_pred", "pred": "y_pred", "predicho": "y_pred",
        "pred_muertes100k": "y_pred", "pred_muertes_100k": "y_pred",
        "predicho_muertes100k": "y_pred",

        "abs_error": "abs_error", "abserror": "abs_error", "abs_err": "abs_error",
        "sq_error": "sq_error", "sqerr": "sq_error", "se2": "sq_error",
        "ano": "a√±o", "anio": "a√±o", "year": "a√±o", "a_o": "a√±o",
        "fold": "fold"
    }
    # tambi√©n intentamos mapear t√≠tulos largos t√≠picos
    long_to_key = {
        "real_(muertes/100k)": "y_real",
        "predicho_(muertes/100k)": "y_pred"
    }

    newcols = {}
    for c in df.columns:
        k = to_key(c)
        k = long_to_key.get(k, keymap.get(k, k))
        newcols[c] = k
    df = df.rename(columns=newcols)
    return df

# ---- Cargar datos ----
df_summary = None
source_used = None

if isinstance(SUMMARY_CSV, str) and SUMMARY_CSV.strip() == "":
    SUMMARY_CSV = None

# 1) Resumen directo si existe
if SUMMARY_CSV and os.path.exists(SUMMARY_CSV):
    df_summary = read_csv_auto(SUMMARY_CSV)
    source_used = os.path.basename(SUMMARY_CSV)
else:
    # 2) Reconstruir desde predicciones con fallbacks de ruta
    cand_paths = [
        PREDICTIONS_CSV,
        f"./LSTM-OPT-15/predicciones_fold{FOLD_TO_PLOT}.csv",
        f"./LSTM-OPT-15/best_models/predicciones_fold{FOLD_TO_PLOT}.csv",
        f"./predicciones_fold{FOLD_TO_PLOT}.csv",
    ]
    pred_path = next((p for p in cand_paths if p and os.path.exists(p)), None)
    if pred_path is None:
        raise FileNotFoundError("No hay SUMMARY_CSV ni PREDICTIONS_CSV v√°lidos. Indica al menos uno.")
    df_pred = read_csv_auto(pred_path)
    if df_pred.empty:
        raise RuntimeError("El CSV de predicciones est√° vac√≠o.")

    # Normalizar nombres de columnas y aceptar sin√≥nimos
    df_pred = normalize_pred_columns(df_pred)

    # Filtrar por fold si est√° la columna
    if "fold" in df_pred.columns:
        df_pred = df_pred[df_pred["fold"] == FOLD_TO_PLOT].copy()
        if df_pred.empty:
            raise RuntimeError(f"No hay filas del fold {FOLD_TO_PLOT} en el CSV de predicciones.")

    # Comprobar columnas m√≠nimas
    req_cols = {"y_real", "y_pred", "provincia"}
    if not req_cols.issubset(set(df_pred.columns)):
        raise ValueError(f"Faltan columnas en predicciones. Requeridas: {req_cols}. "
                         f"Columnas disponibles: {list(df_pred.columns)}")

    # Errores si faltan
    if "abs_error" not in df_pred.columns:
        df_pred["abs_error"] = (df_pred["y_real"] - df_pred["y_pred"]).abs()
    if "sq_error" not in df_pred.columns:
        df_pred["sq_error"]  = (df_pred["y_real"] - df_pred["y_pred"])**2

    # Resumen por provincia
    df_summary = (df_pred.groupby("provincia", as_index=False)
                  .agg(real_avg=("y_real","mean"),
                       pred_avg=("y_pred","mean"),
                       mae=("abs_error","mean"),
                       rmse=("sq_error", lambda s: np.sqrt(np.mean(s))),
                       n=("y_real","count")))
    source_used = os.path.basename(pred_path)
    print(f"üßÆ Resumen reconstruido desde: {source_used}")

df_summary["provincia"] = df_summary["provincia"].astype(str)

# ---- M√©tricas globales del fold (si hay predicciones) ----
try:
    if 'df_pred' not in locals():
        # recarga para m√©tricas si no qued√≥ en memoria
        for p in [PREDICTIONS_CSV,
                  f"./LSTM-OPT-15/predicciones_fold{FOLD_TO_PLOT}.csv",
                  f"./LSTM-OPT-15/best_models/predicciones_fold{FOLD_TO_PLOT}.csv",
                  f"./predicciones_fold{FOLD_TO_PLOT}.csv"]:
            if p and os.path.exists(p):
                df_pred = read_csv_auto(p)
                df_pred = normalize_pred_columns(df_pred)
                break
    if 'df_pred' in locals() and not df_pred.empty:
        if "fold" in df_pred.columns:
            df_pred = df_pred[df_pred["fold"] == FOLD_TO_PLOT].copy()
        if {"y_real","y_pred"}.issubset(df_pred.columns):
            y, yhat = df_pred["y_real"].values, df_pred["y_pred"].values
            rmse_g = float(np.sqrt(np.mean((y - yhat)**2)))
            mae_g  = float(np.mean(np.abs(y - yhat)))
            r2_g   = float(1.0 - np.sum((y - yhat)**2) / np.sum((y - y.mean())**2))
            print(f"üìà M√©tricas globales fold {FOLD_TO_PLOT}: RMSE={rmse_g:.3f}  MAE={mae_g:.3f}  R¬≤={r2_g:.3f}")
except Exception as e:
    print("‚ö†Ô∏è No se pudieron calcular m√©tricas globales:", e)

# ---- Gr√°ficos ----
fig, axs = plt.subplots(1, 2, figsize=(15, 6))

# A) Dispersi√≥n promedio real vs predicho por provincia
sns.scatterplot(data=df_summary, x="real_avg", y="pred_avg",
                ax=axs[0], color="crimson", s=80)
min_lim = min(df_summary["real_avg"].min(), df_summary["pred_avg"].min())
max_lim = max(df_summary["real_avg"].max(), df_summary["pred_avg"].max())
axs[0].plot([min_lim, max_lim], [min_lim, max_lim], "--", color="gray", alpha=0.8)
for _, row in df_summary.iterrows():
    axs[0].text(row["real_avg"]+0.3, row["pred_avg"], row["provincia"], fontsize=8)
axs[0].set_title(f"Promedio real vs predicho por provincia ‚Äî Fold {FOLD_TO_PLOT}")
axs[0].set_xlabel("Real promedio (muertes/100k)")
axs[0].set_ylabel("Predicho promedio (muertes/100k)")
axs[0].grid(True)

# B) Barras de MAE por provincia
df_plot = df_summary.sort_values("mae", ascending=not ORDER_DESC)
sns.barplot(data=df_plot, y="provincia", x="mae", color="steelblue", ax=axs[1])
axs[1].set_title(f"MAE por provincia ‚Äî Fold {FOLD_TO_PLOT}")
axs[1].set_xlabel("MAE (muertes/100k)")
axs[1].set_ylabel("Provincia")
axs[1].grid(True, axis="x")

plt.tight_layout()
if SAVE_PNG:
    os.makedirs(os.path.dirname(OUT_PNG), exist_ok=True)
    plt.savefig(OUT_PNG, dpi=150, bbox_inches="tight")
    print("üñºÔ∏è Figura guardada en:", OUT_PNG)
plt.show()

# ---- Tablas r√°pidas (mejores/peores por MAE) ----
mejores = df_summary.nsmallest(TOP_LABELS, "mae")[["provincia","mae","rmse","n"]]
peores  = df_summary.nlargest(TOP_LABELS, "mae")[["provincia","mae","rmse","n"]]
print("\nüèÖ Mejores provincias (menor MAE):");  display(mejores)
# print("\n‚ö†Ô∏è Peores provincias (mayor MAE):");   display(peores)

"""# BLOQUE 9 ‚Äî Entrenamiento final √∫nico con todos los datos (sin CV)

este bloque funciona con los mejores hiperpar√°metros sobre todos los datos
"""

# ===============================================================
# BLOQUE 9 ‚Äî Entrenamiento final √∫nico con todos los datos (sin CV)
# (con mejoras: shuffle con pesos, guardado de escaladores/config,
#  y guardado/plot de la curva de entrenamiento)
# ===============================================================

from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import os, json, pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# üìÅ Directorio de salida
final_model_path = os.path.join(MODEL_DIR, "modelo_final_entrenado-11keras")
plots_dir = os.path.join(MODEL_DIR, "plots")
os.makedirs(plots_dir, exist_ok=True)

# ===============================
# ‚ú® Escalado de X e y
# ===============================

# Escalador de X con la mejor config
xscaler = get_scaler(SCALER_TYPE).fit(X_seq.reshape(-1, X_seq.shape[2]))
X_seq_sc = xscaler.transform(X_seq.reshape(-1, X_seq.shape[2])).reshape(X_seq.shape)

# Escalador de y con la config establecida
yscaler = make_y_scaler(TARGET_TRANSFORM)
yscaler.fit(y_vec.reshape(-1, 1))
y_vec_t = yscaler.transform(y_vec.reshape(-1, 1)).ravel()

# Pesos por provincia (opcional)
w_train = make_province_weights(groups, WEIGHT_POWER) if WEIGHT_BY_PROVINCE else None

# ===============================
# üß† Entrenamiento del modelo final
# ===============================
tf.keras.backend.clear_session()
model_final = make_model((X_seq.shape[1], X_seq.shape[2]), best_cfg)

callbacks = [EarlyStopping(monitor="loss", patience=PATIENCE_FINAL, restore_best_weights=True)]
if USE_REDUCE_LR:
    callbacks.append(ReduceLROnPlateau(monitor="loss", factor=RLROP_FACTOR,
                                       patience=RLROP_PATIENCE, min_lr=RLROP_MIN_LR, verbose=0))

if USE_TFDATA:
    if WEIGHT_BY_PROVINCE and w_train is not None:
        # ‚úÖ MEJORA: barajar tambi√©n cuando hay sample_weight
        train_ds = (tf.data.Dataset
                    .from_tensor_slices((X_seq_sc, y_vec_t, w_train))
                    .shuffle(len(y_vec_t), reshuffle_each_iteration=True)
                    .batch(BS)
                    .prefetch(tf.data.AUTOTUNE))
    else:
        train_ds = make_ds(X_seq_sc, y_vec_t, BS, shuffle=True)
    history = model_final.fit(train_ds, epochs=FINAL_EPOCHS, verbose=1, callbacks=callbacks)
else:
    history = model_final.fit(X_seq_sc, y_vec_t,
                              epochs=FINAL_EPOCHS, batch_size=BS, verbose=1,
                              callbacks=callbacks,
                              sample_weight=(w_train if WEIGHT_BY_PROVINCE else None))

# ===============================
# üíæ Guardar modelo + escaladores + config + curva
# ===============================
model_final.save(final_model_path)
print("\n‚úÖ Modelo final guardado en:", final_model_path)

# ‚úÖ MEJORA: guardar escaladores y metadatos de config
scalers_path = os.path.join(MODEL_DIR, "scalers_final.pkl")
with open(scalers_path, "wb") as f:
    pickle.dump({
        "xscaler": xscaler,
        "yscaler": yscaler,
        "TARGET_TRANSFORM": TARGET_TRANSFORM,
        "SCALER_TYPE": SCALER_TYPE
    }, f)
print("‚úÖ Escaladores guardados en:", scalers_path)

# ‚úÖ MEJORA: guardar la mejor config usada (si existe)
try:
    cfg_path = os.path.join(MODEL_DIR, "best_cfg_final.json")
    with open(cfg_path, "w") as f:
        json.dump(best_cfg, f, indent=2)
    print("‚úÖ Config del modelo guardada en:", cfg_path)
except Exception as e:
    print("‚ö†Ô∏è No se pudo guardar best_cfg:", e)

# ‚úÖ MEJORA: guardar curva de entrenamiento (CSV + PNG)
hist_df = pd.DataFrame(history.history)
hist_csv = os.path.join(plots_dir, "history_final.csv")
hist_df.to_csv(hist_csv, index=False)
print("‚úÖ History guardado en:", hist_csv)

plt.figure(figsize=(10,4.5))
plt.plot(hist_df.get("loss", []), label="loss")
if "mae" in hist_df.columns:
    plt.plot(hist_df["mae"], label="mae")
plt.title("Curva de entrenamiento (modelo final)")
plt.xlabel("√âpoca"); plt.grid(True); plt.legend()
hist_png = os.path.join(plots_dir, "history_final.png")
plt.tight_layout(); plt.savefig(hist_png, dpi=150)
plt.close()
print("üñºÔ∏è Curva de entrenamiento guardada en:", hist_png)

# ===============================
# üîÆ Funci√≥n para predicciones reales con el modelo final
# ===============================
def predecir_con_modelo_final(X_real):
    """
    Recibe datos secuenciales X_real (shape: [n, lookback, features]) y
    devuelve predicciones desescaladas del modelo final entrenado.
    """
    X_real_sc = xscaler.transform(X_real.reshape(-1, X_real.shape[2])).reshape(X_real.shape)
    preds_t = model_final.predict(X_real_sc, verbose=0).ravel()
    return yscaler.inverse_transform(preds_t.reshape(-1, 1)).ravel()

# ‚úÖ Ejemplo:
# y_pred = predecir_con_modelo_final(X_futuro)

"""# BLOQUE 10 ‚Äî Visualizaci√≥n del modelo final"""

# ============================================================
# BLOQUE 10 ‚Äî Visualizaci√≥n de loss y mae del modelo final (MAE desescalado)
# ============================================================
import os, pickle, numpy as np, pandas as pd, matplotlib.pyplot as plt

# ‚öôÔ∏è Par√°metros
FIGSIZE   = (14, 5)
SAVE_FIG  = True
PLOT_PATH = os.path.join(MODEL_DIR, "curva_entrenamiento_modelo_final.png")

# üìà Cargar history
df_hist_final = None
if 'history' in globals() and hasattr(history, 'history'):
    df_hist_final = pd.DataFrame(history.history)
else:
    hist_csv = os.path.join(MODEL_DIR, "plots", "history_final.csv")
    if not os.path.exists(hist_csv):
        raise FileNotFoundError(
            "No encuentro 'history' en memoria ni el CSV con la curva. "
            f"Ejecuta el Bloque 9 o crea el archivo: {hist_csv}"
        )
    df_hist_final = pd.read_csv(hist_csv)

# A√±adir columna epoch si no existe
if "epoch" not in df_hist_final.columns:
    df_hist_final["epoch"] = np.arange(1, len(df_hist_final) + 1)

# =========================
# üîÑ Desescalar el MAE
# =========================
mae_label = "MAE (original)"
df_hist_final["mae_original"] = np.nan

# 1) Intentar obtener yscaler de memoria o de disco
yscaler_obj = None
transform_kind = None
if 'yscaler' in globals():
    yscaler_obj = yscaler
    # si tienes la variable global TARGET_TRANSFORM, √∫sala
    if 'TARGET_TRANSFORM' in globals():
        transform_kind = TARGET_TRANSFORM
else:
    scalers_path = os.path.join(MODEL_DIR, "scalers_final.pkl")
    if os.path.exists(scalers_path):
        with open(scalers_path, "rb") as f:
            pack = pickle.load(f)
        yscaler_obj = pack.get("yscaler", None)
        transform_kind = pack.get("TARGET_TRANSFORM", None)

# 2) Convertir MAE si existe la columna 'mae'
if "mae" in df_hist_final.columns:
    if (transform_kind is None) or (str(transform_kind).lower() == "none"):
        # sin transformaci√≥n ‚Üí ya est√° en escala original
        df_hist_final["mae_original"] = df_hist_final["mae"]
    elif str(transform_kind).lower() == "zscore":
        # y_t = (y - mu)/sigma ‚áí |y - ≈∑| = sigma * |y_t - ≈∑_t|
        if yscaler_obj is not None and hasattr(yscaler_obj, "scale_"):
            sigma = float(np.ravel(yscaler_obj.scale_)[0])
            df_hist_final["mae_original"] = df_hist_final["mae"] * sigma
        else:
            print("‚ö†Ô∏è No se encontr√≥ 'yscaler.scale_' para desescalar; se muestra MAE transformado.")
            df_hist_final["mae_original"] = df_hist_final["mae"]
    elif str(transform_kind).lower() == "log1p":
        # No es lineal: no se puede desescalar MAE por √©poca con precisi√≥n solo con el history
        print("‚ö†Ô∏è TARGET_TRANSFORM='log1p': no se puede desescalar el MAE por √©poca con exactitud solo con el history.")
        df_hist_final["mae_original"] = np.nan
        mae_label = "MAE (transformado)"
    else:
        print(f"‚ö†Ô∏è Transformaci√≥n de y desconocida: {transform_kind}. Se mostrar√° MAE transformado.")
        df_hist_final["mae_original"] = df_hist_final["mae"]
else:
    print("‚ö†Ô∏è El history no contiene la m√©trica 'mae'.")

# üé® Graficar
fig, axs = plt.subplots(1, 2, figsize=FIGSIZE)

# Subplot 1: LOSS
axs[0].plot(df_hist_final["epoch"], df_hist_final["loss"], marker="o")
# marcar mejor √©poca (m√≠nimo loss)
best_i   = df_hist_final["loss"].idxmin()
best_ep  = int(df_hist_final.loc[best_i, "epoch"])
best_val = float(df_hist_final.loc[best_i, "loss"])
axs[0].scatter([best_ep], [best_val], s=40, zorder=5)
axs[0].axvline(best_ep, color="gray", alpha=0.3, linestyle="--")
axs[0].set_title("Evoluci√≥n de la p√©rdida (loss)")
axs[0].set_xlabel("√âpoca")
axs[0].set_ylabel("Loss")
axs[0].grid(True)

# Subplot 2: MAE desescalado (si disponible)
if "mae" in df_hist_final.columns:
    if df_hist_final["mae_original"].notna().any():
        axs[1].plot(df_hist_final["epoch"], df_hist_final["mae_original"], marker="s")
        axs[1].set_ylabel(f"{mae_label} (muertes/100k)" if mae_label=="MAE (original)" else mae_label)
    else:
        axs[1].plot(df_hist_final["epoch"], df_hist_final["mae"], marker="s")
        axs[1].set_ylabel("MAE (transformado)")
    axs[1].set_title("Evoluci√≥n del MAE")
    axs[1].set_xlabel("√âpoca")
    axs[1].grid(True)
else:
    axs[1].axis("off")
    axs[1].text(0.5, 0.5, "Sin m√©trica 'mae' en history", ha="center", va="center")

plt.suptitle("Curvas de entrenamiento del modelo final")
plt.tight_layout()

# üíæ Guardar
if SAVE_FIG:
    os.makedirs(os.path.dirname(PLOT_PATH), exist_ok=True)
    plt.savefig(PLOT_PATH, dpi=150, bbox_inches="tight")
    print("‚úÖ Figura guardada en:", PLOT_PATH)

plt.show()

#===============================================================
# BLOQUE 10b ‚Äî Visualizaci√≥n del modelo final
# (con mejoras: l√≠mites coherentes en scatter, residuales y m√©tricas ponderadas)
# ===============================================================

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
import pandas as pd

# ===============================
# üî¢ Predicciones sobre X_seq
# ===============================
X_seq_sc = xscaler.transform(X_seq.reshape(-1, X_seq.shape[2])).reshape(X_seq.shape)
y_pred_t = model_final.predict(X_seq_sc, verbose=0).ravel()
y_pred = yscaler.inverse_transform(y_pred_t.reshape(-1, 1)).ravel()

# ===============================
# üìä M√©tricas globales (no ponderadas)
# ===============================
rmse = mean_squared_error(y_vec, y_pred) ** 0.5
mae  = mean_absolute_error(y_vec, y_pred)
r2   = r2_score(y_vec, y_pred)
print(f"\nEvaluaci√≥n global del modelo final (no ponderada):")
print(f"RMSE: {rmse:.3f} | MAE: {mae:.3f} | R¬≤: {r2:.3f}")

# ‚úÖ M√©tricas ponderadas por provincia (si procede)
w_eval = None
if 'groups' in globals():
    try:
        if 'WEIGHT_BY_PROVINCE' in globals() and WEIGHT_BY_PROVINCE:
            w_eval = make_province_weights(groups, WEIGHT_POWER)
    except Exception:
        w_eval = None

if w_eval is not None:
    rmse_w = mean_squared_error(y_vec, y_pred, sample_weight=w_eval) ** 0.5
    mae_w  = mean_absolute_error(y_vec, y_pred, sample_weight=w_eval)
    try:
        r2_w = r2_score(y_vec, y_pred, sample_weight=w_eval)
    except TypeError:
        r2_w = np.nan
    print(f"Evaluaci√≥n ponderada por provincia:")
    print(f"RMSE_w: {rmse_w:.3f} | MAE_w: {mae_w:.3f} | R¬≤_w: {r2_w:.3f}")

# ===============================
# üåê Scatter real vs. predicho (l√≠mites coherentes)
# ===============================
plt.figure(figsize=(7, 7))
plt.scatter(y_vec, y_pred, alpha=0.3)
lo = float(min(y_vec.min(), y_pred.min()))
hi = float(max(y_vec.max(), y_pred.max()))
plt.plot([lo, hi], [lo, hi], 'r--', label="Ideal")
plt.xlim(lo, hi); plt.ylim(lo, hi)
plt.xlabel("Muertes real (por 100k)")
plt.ylabel("Muertes predicho (por 100k)")
plt.title("Real vs. Predicho Modelo final")
plt.grid(True)
plt.legend()
plt.axis("equal")
plt.tight_layout()
plt.show()

# ===============================
# ‚ôªÔ∏è Residuales (pred - real)
# ===============================
res = y_pred - y_vec
plt.figure(figsize=(8, 4))
plt.scatter(y_vec, res, alpha=0.3)
plt.axhline(0.0, color='red', linestyle='--', label='Residual = 0')
plt.xlabel("Muertes real (por 100k)")
plt.ylabel("Residual (pred ‚àí real)")
plt.title("Residuals vs Real (Modelo final, in-sample)")
plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()

plt.figure(figsize=(8, 4))
sns.histplot(res, bins=30, kde=True)
plt.axvline(0.0, color='red', linestyle='--')
plt.xlabel("Residual (pred ‚àí real)")
plt.ylabel("Frecuencia")
plt.title("Distribuci√≥n de residuales")
plt.grid(True); plt.tight_layout(); plt.show()

# ===============================
# üìä Distribuci√≥n del error absoluto
# ===============================
abs_err = np.abs(y_vec - y_pred)
plt.figure(figsize=(8, 4))
sns.histplot(abs_err, bins=30, kde=True)
plt.xlabel("Error absoluto (por 100k)")
plt.ylabel("Frecuencia")
plt.title("Distribuci√≥n del error absoluto")
plt.grid(True)
plt.tight_layout()
plt.show()

# ===============================
# üåç Error por provincia (opcional)
# ===============================
if 'groups' in globals():
    df_err = pd.DataFrame({"province": groups, "y_true": y_vec, "y_pred": y_pred})
    df_err["abs_err"] = np.abs(df_err["y_true"] - df_err["y_pred"])
    plt.figure(figsize=(10, 4))
    sns.boxplot(data=df_err, x="province", y="abs_err")
    plt.xticks(rotation=90)
    plt.xlabel("Provincia"); plt.ylabel("Error absoluto (por 100k)")
    plt.title("Distribuci√≥n del error absoluto por provincia")
    plt.tight_layout()
    plt.show()
else:
    print("‚ö†Ô∏è Variable 'groups' no disponible. No se puede hacer boxplot por provincia.")

# ===============================================================
# BLOQUE 10C ‚Äî TODAS las provincias usadas en el BLOQUE 9 (modelo final)
# Dibuja:
#   1) Dispersi√≥n del promedio real vs. predicho por provincia
#   2) Barras de MAE por provincia (todas)
# Usa el modelo final entrenado (model_final) y los mismos escaladores del BLOQUE 9.
# ===============================================================

import os, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns

# -------- CONFIG --------
SAVE_PNG        = True
OUT_DIR         = os.path.join(MODEL_DIR, "plots")
SCATTER_FNAME   = "final_scatter_promedio_all_provincias.png"
BARS_FNAME      = "final_mae_all_provincias.png"
LABEL_POINTS    = True     # etiqueta todos los puntos en el scatter
ORDER_DESC      = True     # barras ordenadas de peor‚Üímejor (MAE alto a bajo)
POINT_SIZE      = 55
# ------------------------

os.makedirs(OUT_DIR, exist_ok=True)

# ------------------------------
# 1) Predicciones del modelo final sobre TODO X_seq (como en 10b)
# ------------------------------
# Reutiliza y_pred si ya existe; si no, lo calcula.
if 'y_pred' not in globals():
    X_seq_sc = xscaler.transform(X_seq.reshape(-1, X_seq.shape[2])).reshape(X_seq.shape)
    y_pred_t = model_final.predict(X_seq_sc, verbose=0).ravel()
    y_pred = yscaler.inverse_transform(y_pred_t.reshape(-1, 1)).ravel()

# Comprobaciones m√≠nimas
if 'groups' not in globals():
    raise RuntimeError("No encuentro 'groups'. Es necesario para saber qu√© provincias se usaron en el BLOQUE 9.")

# ------------------------------
# 2) Resumen por provincia (TODAS las provincias de BLOQUE 9)
# ------------------------------
df_all = pd.DataFrame({
    "provincia": pd.Series(groups, dtype=str),
    "y_real":    y_vec,
    "y_pred":    y_pred
})
df_all["abs_error"] = np.abs(df_all["y_real"] - df_all["y_pred"])
df_all["sq_error"]  = (df_all["y_real"] - df_all["y_pred"])**2

df_summary = (df_all.groupby("provincia", as_index=False)
              .agg(real_avg=("y_real","mean"),
                   pred_avg=("y_pred","mean"),
                   mae=("abs_error","mean"),
                   rmse=("sq_error", lambda s: float(np.sqrt(np.mean(s)))),
                   n=("y_real","count")))

# ------------------------------
# 3) Dispersi√≥n: promedio real vs predicho (TODAS)
# ------------------------------
plt.figure(figsize=(9, 8))
ax = sns.scatterplot(data=df_summary, x="real_avg", y="pred_avg",
                     s=POINT_SIZE, color="#d62728", edgecolor="none")
lo = min(df_summary["real_avg"].min(), df_summary["pred_avg"].min())
hi = max(df_summary["real_avg"].max(), df_summary["pred_avg"].max())
plt.plot([lo, hi], [lo, hi], "--", color="gray", alpha=0.8)
plt.xlabel("Real promedio (muertes/100k)")
plt.ylabel("Predicho promedio (muertes/100k)")
plt.title("Promedio real vs predicho por provincia  Modelo final")
plt.grid(True)

if LABEL_POINTS:
    for _, r in df_summary.iterrows():
        ax.text(r["real_avg"]+0.35, r["pred_avg"], r["provincia"], fontsize=8)

plt.tight_layout()
if SAVE_PNG:
    out = os.path.join(OUT_DIR, SCATTER_FNAME)
    plt.savefig(out, dpi=150, bbox_inches="tight")
    print("üñºÔ∏è Scatter guardado en:", out)
plt.show()

# ------------------------------
# 4) Barras de MAE por provincia (TODAS)
# ------------------------------
df_plot = df_summary.sort_values("mae", ascending=not ORDER_DESC)
height = max(6, 0.22 * len(df_plot))  # altura din√°mica
plt.figure(figsize=(12, height))
sns.barplot(data=df_plot, y="provincia", x="mae", color="steelblue")
plt.xlabel("MAE (muertes/100k)")
plt.ylabel("Provincia")
plt.title("MAE por provincia  Modelo final")
plt.grid(True, axis="x", linestyle="--", alpha=0.35)
plt.tight_layout()
if SAVE_PNG:
    out = os.path.join(OUT_DIR, BARS_FNAME)
    plt.savefig(out, dpi=150, bbox_inches="tight")
    print("üñºÔ∏è Barras guardadas en:", out)
plt.show()

# ===============================================================
# BLOQUE 10D ‚Äî M√©tricas detalladas (global, por provincia y a√±o)
#              + reconstrucci√≥n de prov/year + opci√≥n de pesos
# ===============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator


# seaborn es opcional para plots bonitos
try:
    import seaborn as sns
    _HAS_SNS = True
except Exception:
    _HAS_SNS = False

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# -------------------------
# ‚öôÔ∏è CONFIG (ajusta si hace falta)
# -------------------------
SRC_DF    = df_pa        # panel con columnas province/year usado en BLOQUE 2
COL_PROV  = "province"
COL_YEAR  = "year"

# Pesos por provincia (opcional)
USAR_PESOS    = False        # ‚¨ÖÔ∏è pon True si quieres m√©tricas ponderadas
PESO_MODO     = "inv_n"      # "inv_n" (cada provincia pesa igual) | "custom"
PESOS_PROV    = {}           # dict opcional: {"Madrid": 2.0, "Soria": 0.5, ...}

# Deben existir en el entorno: LOOKBACK_L, HORIZON_H, y_vec (real, desescalado), y_pred (desescalado)
# ---------------------------------------------------------------

# === Helpers de compatibilidad y m√©tricas ===
def rmse_safe(y_true, y_pred):
    """RMSE compatible con todas las versiones de sklearn."""
    try:
        return mean_squared_error(y_true, y_pred, squared=False)
    except TypeError:
        return np.sqrt(mean_squared_error(y_true, y_pred))

def mae_weighted(y_true, y_pred, w=None):
    if w is None:
        return mean_absolute_error(y_true, y_pred)
    w = np.asarray(w, dtype=float)
    e = np.abs(np.asarray(y_true) - np.asarray(y_pred))
    return np.average(e, weights=w)

def rmse_weighted(y_true, y_pred, w=None):
    if w is None:
        return rmse_safe(y_true, y_pred)
    w = np.asarray(w, dtype=float)
    se = (np.asarray(y_true) - np.asarray(y_pred))**2
    return np.sqrt(np.average(se, weights=w))

def r2_weighted_safe(y_true, y_pred, w=None):
    """R¬≤ ponderado (compatibilidad). Si no hay pesos, usa sklearn."""
    if w is None:
        return r2_score(y_true, y_pred)
    w = np.asarray(w, dtype=float)
    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)
    # Definici√≥n de R¬≤ ponderado: 1 - SS_res / SS_tot, con medias ponderadas
    y_bar = np.average(y_true, weights=w)
    ss_res = np.sum(w * (y_true - y_pred)**2)
    ss_tot = np.sum(w * (y_true - y_bar)**2)
    return 1.0 - (ss_res / ss_tot if ss_tot > 0 else np.nan)

# === Reconstrucci√≥n de prov_seq / year_seq SIN tocar BLOQUE 2 ===
def rebuild_meta_sequences(panel_df, lookback, horizon, col_prov=COL_PROV, col_year=COL_YEAR):
    """Devuelve arrays prov_seq, year_seq alineados con y_vec/y_pred."""
    prov_seq, year_seq = [], []
    panel_df = panel_df.sort_values([col_prov, col_year]).reset_index(drop=True)
    for prov, g in panel_df.groupby(col_prov, sort=True):
        g = g.sort_values(col_year)
        n = len(g)
        max_i = n - lookback - horizon + 1
        if max_i <= 0:
            continue
        for i in range(max_i):
            label_idx = i + lookback + horizon - 1
            prov_seq.append(prov)
            year_seq.append(int(g.iloc[label_idx][col_year]))
    return np.asarray(prov_seq), np.asarray(year_seq)

prov_seq, year_seq = rebuild_meta_sequences(SRC_DF, LOOKBACK_L, HORIZON_H, COL_PROV, COL_YEAR)

# Alineaci√≥n segura con y_vec / y_pred
min_len = min(len(prov_seq), len(y_vec), len(y_pred))
if len(prov_seq) != len(y_vec) or len(y_vec) != len(y_pred):
    print(f"‚ö†Ô∏è Alineaci√≥n: recortando al m√≠nimo com√∫n ({min_len}). "
          f"prov_seq={len(prov_seq)}, y_vec={len(y_vec)}, y_pred={len(y_pred)}")
prov_seq = prov_seq[:min_len]
year_seq = year_seq[:min_len]
y_true   = np.asarray(y_vec)[:min_len]
y_hat    = np.asarray(y_pred)[:min_len]

# Construir DF de evaluaci√≥n
df_eval = pd.DataFrame({
    COL_PROV: prov_seq,
    COL_YEAR: year_seq,
    "real":   y_true,
    "pred":   y_hat
})

# === Pesos por muestra (si procede) ===
sample_w = None
if USAR_PESOS:
    if PESO_MODO == "inv_n":
        # Cada provincia contribuye igual: peso_i = 1 / n_provincia
        n_por_prov = df_eval.groupby(COL_PROV).size().to_dict()
        sample_w = df_eval[COL_PROV].map(lambda p: 1.0 / n_por_prov[p]).to_numpy(dtype=float)
        # Normaliza para que sumen N (opcional; no cambia RMSE/MAE ponderados por media)
        sample_w = sample_w * (len(sample_w) / sample_w.sum())
    elif PESO_MODO == "custom" and PESOS_PROV:
        # Pesos directos por provincia
        sample_w = df_eval[COL_PROV].map(lambda p: PESOS_PROV.get(p, 1.0)).to_numpy(dtype=float)
    else:
        print("‚ö†Ô∏è USAR_PESOS=True pero no se reconoce PESO_MODO; se calcular√° sin pesos.")
        sample_w = None

# -------------------------
# 1) M√©tricas globales
# -------------------------
rmse_g = rmse_weighted(df_eval["real"], df_eval["pred"], sample_w)
mae_g  = mae_weighted(df_eval["real"], df_eval["pred"], sample_w)
r2_g   = r2_weighted_safe(df_eval["real"], df_eval["pred"], sample_w)
print("\nüîé M√©tricas globales" + (" (ponderadas)" if USAR_PESOS and sample_w is not None else ""))
print(f"RMSE: {rmse_g:.3f} | MAE: {mae_g:.3f} | R¬≤: {r2_g:.3f}")

# -------------------------
# 2) M√©tricas por provincia
# -------------------------
def _metrics_group(g):
    # Si hay pesos, extrae los de ese grupo
    w = None
    if sample_w is not None:
        w = sample_w[g.index]
    return pd.Series({
        "RMSE": rmse_weighted(g["real"].to_numpy(), g["pred"].to_numpy(), w),
        "MAE":  mae_weighted(g["real"].to_numpy(), g["pred"].to_numpy(), w),
        "R2":   r2_weighted_safe(g["real"].to_numpy(), g["pred"].to_numpy(), w if len(g) > 1 else None)
    })

prov_metrics = df_eval.groupby(COL_PROV, sort=True).apply(_metrics_group).reset_index()

print("\nM√©tricas por provincia:")
try:
    display(prov_metrics.sort_values("RMSE"))
except Exception:
    print(prov_metrics.sort_values("RMSE"))

# Plot RMSE por provincia
plt.figure(figsize=(12, 8))
orden = prov_metrics.sort_values("RMSE", ascending=True)
plt.barh(orden[COL_PROV], orden["RMSE"], color="steelblue")

plt.title("RMSE por provincia Modelo final", fontsize=11)
plt.xlabel("RMSE (Error cuadr√°tico medio en muertes respiratorias por 100k)")
plt.ylabel("Provincia")
plt.tight_layout()
plt.show()


# -------------------------
# 3) M√©tricas por a√±o
# -------------------------
def _metrics_group_year(g):
    w = None
    if sample_w is not None:
        w = sample_w[g.index]
    return pd.Series({
        "RMSE": rmse_weighted(g["real"].to_numpy(), g["pred"].to_numpy(), w),
        "MAE":  mae_weighted(g["real"].to_numpy(), g["pred"].to_numpy(), w),
        "R2":   r2_weighted_safe(g["real"].to_numpy(), g["pred"].to_numpy(), w if len(g) > 1 else None)
    })

year_metrics = df_eval.groupby(COL_YEAR, sort=True).apply(_metrics_group_year).reset_index()

print("\nM√©tricas por a√±o:")
try:
    display(year_metrics.sort_values(COL_YEAR))
except Exception:
    print(year_metrics.sort_values(COL_YEAR))

# Plot evoluci√≥n de errores por a√±o
plt.figure(figsize=(10, 5))
plt.plot(year_metrics[COL_YEAR], year_metrics["RMSE"], marker="o", label="RMSE")
plt.plot(year_metrics[COL_YEAR], year_metrics["MAE"],  marker="s", label="MAE")
plt.title("Evoluci√≥n del error por a√±o", fontsize=11)
plt.xlabel("A√±o")
plt.ylabel("Error (muertes respiratorias por 100k)")
plt.legend(title="M√©tricas", loc="upper right")
plt.gca().xaxis.set_major_locator(MultipleLocator(2))
plt.tight_layout()
plt.show()

# -------------------------
# 4) Scatter global Real vs Predicho
# -------------------------
plt.figure(figsize=(6, 6))
if _HAS_SNS:
    sns.scatterplot(x=df_eval["real"], y=df_eval["pred"], hue=df_eval[COL_PROV], legend=False, s=20)
else:
    plt.scatter(df_eval["real"], df_eval["pred"], s=12, alpha=0.7)

mn, mx = df_eval["real"].min(), df_eval["real"].max()
plt.plot([mn, mx], [mn, mx], "k--", lw=2, label="L√≠nea perfecta (y=x)")

plt.title("Real vs. Predicho (todas las provincias)", fontsize=10)
plt.xlabel("Valores reales (muertes respiratorias por 100k)")
plt.ylabel("Valores predichos (muertes respiratorias por 100k)")
plt.legend(loc="upper left")
plt.tight_layout()
plt.show()

"""# Bloque 11 Importancia de las variables"""

# ===============================================================
# BLOQUE 11 ‚Äî Influencia de variables del modelo final
#   (A: Correlaciones Pearson + B: Permutation Importance)
# ===============================================================

import os, numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error

# -----------------------
# ‚öôÔ∏è CONFIG
# -----------------------
OUT_DIR  = os.path.join(MODEL_DIR, "plots")
SAVE_PNG = True
SEED     = 42
rng      = np.random.default_rng(SEED)
os.makedirs(OUT_DIR, exist_ok=True)

# ===============================================================
# üîÅ Funci√≥n auxiliar: predecir con escalado y desescalado correcto
# ===============================================================
def predict_model(X_seq):
    X_sc = xscaler.transform(X_seq.reshape(-1, X_seq.shape[2])).reshape(X_seq.shape)
    y_pred_t = model_final.predict(X_sc, verbose=0).ravel()
    return yscaler.inverse_transform(y_pred_t.reshape(-1,1)).ravel()

# ===============================================================
# A) Correlaciones Pearson (√∫ltimo timestep de cada secuencia)
# ===============================================================
X_last = X_seq[:, -1, :]   # √∫ltimo a√±o en cada ventana
df_corr = pd.DataFrame(X_last, columns=features)
df_corr["target"] = y_vec

corr = df_corr.corr(numeric_only=True)

plt.figure(figsize=(1.0+0.5*len(features), 1.0+0.5*len(features)))
plt.imshow(corr, cmap="coolwarm", vmin=-1, vmax=1)
plt.colorbar(label="Pearson r")
plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
plt.yticks(range(len(corr.index)),  corr.index)
plt.title("BLOQUE 11 ‚Äî Heatmap de correlaciones")
plt.tight_layout()
if SAVE_PNG:
    plt.savefig(os.path.join(OUT_DIR, "B11_corr_heatmap.png"), dpi=150)
plt.show()

corr_target = corr["target"].drop("target").abs().sort_values(ascending=False)
plt.figure(figsize=(10, max(4, 0.35*len(corr_target))))
corr_target.plot(kind="barh")
plt.gca().invert_yaxis()
plt.xlabel("|Correlaci√≥n con target|")
plt.title("BLOQUE 11 ‚Äî Ranking correlacional")
plt.tight_layout()
if SAVE_PNG:
    plt.savefig(os.path.join(OUT_DIR, "B11_corr_ranking.png"), dpi=150)
plt.show()

# ===============================================================
# B) Importancia por Permutaci√≥n (LSTM final)
# ===============================================================
y_pred_base = predict_model(X_seq)
mae_base    = mean_absolute_error(y_vec, y_pred_base)

importancias = []
n, T, F = X_seq.shape
for k, fname in enumerate(features):
    X_perm = X_seq.copy()
    idx = rng.permutation(n)
    X_perm[:, :, k] = X_perm[idx, :, k]   # permutaci√≥n coherente en todos los timesteps
    y_pred_perm = predict_model(X_perm)
    mae_perm    = mean_absolute_error(y_vec, y_pred_perm)
    delta_mae   = mae_perm - mae_base
    importancias.append((fname, float(delta_mae)))

imp_df = pd.DataFrame(importancias, columns=["feature", "delta_MAE"]).sort_values("delta_MAE", ascending=True)

plt.figure(figsize=(10, max(4, 0.35*len(imp_df))))
plt.barh(imp_df["feature"], imp_df["delta_MAE"])
plt.xlabel("ŒîMAE al permutar (‚Üë peor ‚áí ‚Üë importancia)")
plt.title("Importancia por permutaci√≥n")
plt.tight_layout()
if SAVE_PNG:
    plt.savefig(os.path.join(OUT_DIR, "B11_permutation_importance.png"), dpi=150)
plt.show()

# Guardar resultados
imp_df.to_csv(os.path.join(OUT_DIR, "B11_permutation_importance.csv"), index=False)
corr.to_csv(os.path.join(OUT_DIR, "B11_correlations.csv"))

print(f"‚úÖ BLOQUE 11 listo. MAE base: {mae_base:.3f}")

"""# Bloque 12 Importancia de las variables"""

# ===============================================================
# BLOQUE 12 ‚Äî Permutaciones avanzadas (mejorado):
#   (1) Importancia por GRUPOS de variables
#   (2) Importancia por TIMESTEP (t) y por variable (k)
# ===============================================================

import os, numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error

# -----------------------
# ‚öôÔ∏è CONFIG
# -----------------------
MODEL_DIR    = "LSTM-OPT-15"   # carpeta base donde se guardan los plots
OUT_DIR      = os.path.join(MODEL_DIR, "plots")
SAVE_PNG     = True
SEED         = 42
N_REPEATS    = 10          # repeticiones para promediar ruido
SHOW_TOP     = None        # si quieres limitar n¬∫ de barras en los plots, pon un entero
TIMESTEPS    = None        # None = todos; o una lista/array de √≠ndices t a evaluar (0..T-1)

# Grupos sugeridos (aj√∫stalos a tus nombres reales). Se usa la intersecci√≥n con `features`.
GROUPS = {
    "Pollutants":  ["no2", "pm10", "pm2_5", "o3", "so2"],
    "Geography":   ["latitude", "longitude"],
    "Demography":  ["population", "pib"],
    "Seasonality": ["year_sin", "year_cos"],
    # "Meteo": ["temp","wind","humidity"],
}

# Opcionales de est√©tica global (puedes comentar si no los quieres)
plt.rcParams["figure.dpi"] = 120
plt.rcParams["savefig.bbox"] = "tight"

# -----------------------
rng = np.random.default_rng(SEED)
os.makedirs(OUT_DIR, exist_ok=True)

# üîß helper para guardar/mostrar con buen espaciado y cerrar figura
def save_show(fig, filename):
    if SAVE_PNG:
        fig.savefig(os.path.join(OUT_DIR, filename), dpi=150, bbox_inches="tight")
    plt.show()
    plt.close(fig)

# ===============================================================
# üîÅ Auxiliar: predecir con el escalado correcto
# ===============================================================
def predict_model(X_seq):
    X_sc = xscaler.transform(X_seq.reshape(-1, X_seq.shape[2])).reshape(X_seq.shape)
    y_pred_t = model_final.predict(X_sc, verbose=0).ravel()
    return yscaler.inverse_transform(y_pred_t.reshape(-1,1)).ravel()

# ===============================================================
# üìè M√©trica base del modelo
# ===============================================================
y_pred_base = predict_model(X_seq)
mae_base    = mean_absolute_error(y_vec, y_pred_base)
print(f"[B12] MAE base: {mae_base:.4f}")

# ===============================================================
# (1) Importancia por GRUPOS de variables
#   - Permuta simult√°neamente todas las features del grupo
#   - Misma permutaci√≥n para todos los timesteps (coherencia temporal)
#   - Repite N_REPEATS y promedia (con desviaci√≥n est√°ndar)
# ===============================================================
n, T, F = X_seq.shape
name_to_idx = {f: i for i, f in enumerate(features)}

# Intersecci√≥n grupo-features reales; a√±ade grupo "Other" autom√°ticamente
resolved_groups = {}
already = set()
for gname, gvars in GROUPS.items():
    idxs = [name_to_idx[v] for v in gvars if v in name_to_idx]
    if idxs:
        resolved_groups[gname] = idxs
        already.update(idxs)

rest_idxs = [i for i in range(F) if i not in already]
if rest_idxs:
    resolved_groups["Other"] = rest_idxs

group_rows = []
for gname, idxs in resolved_groups.items():
    deltas = []
    for rep in range(N_REPEATS):
        Xp = X_seq.copy()
        idx = rng.permutation(n)
        # misma permutaci√≥n para TODAS las features del grupo y en TODOS los timesteps
        Xp[:, :, idxs] = Xp[idx, :, :][:, :, idxs]
        yp = predict_model(Xp)
        mae_p = mean_absolute_error(y_vec, yp)
        deltas.append(mae_p - mae_base)
    group_rows.append({
        "group": gname,
        "features_in_group": len(idxs),
        "delta_MAE_mean": float(np.mean(deltas)),
        "delta_MAE_std":  float(np.std(deltas, ddof=1))
    })

group_imp = pd.DataFrame(group_rows).sort_values("delta_MAE_mean", ascending=True)
group_imp.to_csv(os.path.join(OUT_DIR, "B12_group_permutation_importance.csv"), index=False)

# Plot grupos (m√°s aire y cierre de figura)
fig, ax = plt.subplots(
    figsize=(12, max(4, 0.55*len(group_imp))),
    constrained_layout=True
)
x = group_imp["delta_MAE_mean"].values
e = group_imp["delta_MAE_std"].values
y = group_imp["group"].values
if SHOW_TOP:
    x, e, y = x[:SHOW_TOP], e[:SHOW_TOP], y[:SHOW_TOP]
ax.barh(y, x, xerr=e, capsize=5)
ax.set_xlabel("ŒîMAE al permutar grupo (media ¬± sd)  ‚Üë peor ‚áí ‚Üë importancia")
ax.set_title("Importancia por grupos (Permutation)")
ax.invert_yaxis()
save_show(fig, "B12_group_permutation_importance.png")

# ===============================================================
# (2) Importancia por TIMESTEP y FEATURE
#   - Permuta SOLO X[:, t, k] manteniendo el resto intacto
#   - Repite N_REPEATS y promedia
#   - Devuelve matriz ŒîMAE de tama√±o (T, F)
# ===============================================================
t_list = list(range(T)) if TIMESTEPS is None else list(TIMESTEPS)

delta_mat = np.zeros((T, F), dtype=np.float32)  # acumulador de medias
for k in range(F):
    for ti, t in enumerate(t_list):
        deltas = []
        for rep in range(N_REPEATS):
            Xp = X_seq.copy()
            idx = rng.permutation(n)
            # solo permuta la "columna" (timestep t, feature k)
            Xp[:, t, k] = Xp[idx, t, k]
            yp = predict_model(Xp)
            mae_p = mean_absolute_error(y_vec, yp)
            deltas.append(mae_p - mae_base)
        delta_mat[t, k] = np.mean(deltas)

# Guardar resultados detallados (larga pero √∫til)
timesteps = np.arange(T)
df_heat = pd.DataFrame(delta_mat, index=timesteps, columns=features)
df_heat.to_csv(os.path.join(OUT_DIR, "B12_timestep_feature_permutation.csv"))

# --- Heatmap (t x features) con espaciamiento mejorado
fig, ax = plt.subplots(
    figsize=(1.4 + 0.55*F, 2.0 + 0.45*len(t_list)),
    constrained_layout=True
)
im = ax.imshow(delta_mat, aspect="auto")
cb = fig.colorbar(im, ax=ax)
cb.set_label("ŒîMAE al permutar (‚Üë peor ‚áí ‚Üë importancia)")
ax.set_yticks(range(T))
ax.set_xticks(range(F))
ax.set_xticklabels(features, rotation=90)
ax.set_title("Importancia por timestep y variable")
save_show(fig, "B12_timestep_feature_heatmap.png")

# --- Agregados √∫tiles
# Importancia media por timestep (promediando features)
t_imp = df_heat.mean(axis=1).to_frame("delta_MAE_mean")
t_imp.index.name = "timestep"
t_imp.to_csv(os.path.join(OUT_DIR, "B12_timestep_mean_importance.csv"))

fig, ax = plt.subplots(figsize=(12, 4), constrained_layout=True)
ax.plot(t_imp.index, t_imp["delta_MAE_mean"], marker="o")
ax.set_xlabel("timestep (0 = a√±o m√°s antiguo)")
ax.set_ylabel("ŒîMAE medio")
ax.set_title("Importancia media por timestep")
save_show(fig, "B12_timestep_mean_importance.png")

# Importancia media por feature (promediando timesteps)
f_imp = df_heat.mean(axis=0).sort_values(ascending=False).to_frame("delta_MAE_mean")
f_imp.index.name = "feature"
f_imp.to_csv(os.path.join(OUT_DIR, "B12_feature_mean_importance_over_timesteps.csv"))

fig, ax = plt.subplots(
    figsize=(12, max(4, 0.45*len(f_imp))),
    constrained_layout=True
)
vals = f_imp["delta_MAE_mean"].values
labs = f_imp.index.values
if SHOW_TOP:
    vals, labs = vals[:SHOW_TOP], labs[:SHOW_TOP]
ax.barh(labs, vals)
ax.invert_yaxis()
ax.set_xlabel("ŒîMAE medio sobre timesteps")
ax.set_title("Importancia media por feature (perm. por timestep)")
save_show(fig, "B12_feature_mean_importance_over_timesteps.png")

print("‚úÖ BLOQUE 12 listo.")
print("   - Guardado: B12_group_permutation_importance(.csv/.png)")
print("   - Guardado: B12_timestep_feature_permutation.csv + heatmap/agrupados")

df_pa

# df_pa.info()

df_pa.describe()